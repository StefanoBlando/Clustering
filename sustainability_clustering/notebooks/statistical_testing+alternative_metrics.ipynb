# =============================================================================
# MODULO TEST STATISTICI RIGOROSI E METRICHE ALTERNATIVE
# Validazione statistica formale dei risultati clustering
# =============================================================================

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from scipy import stats
from scipy.stats import chi2_contingency, kruskal, mannwhitneyu, anderson_ksamp
from scipy.stats import shapiro, levene, bartlett
from statsmodels.stats.multitest import multipletests
from sklearn.model_selection import permutation_test_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO TEST STATISTICI RIGOROSI ===")

# =============================================================================
# STEP 1: PREPARAZIONE DATI PER TEST STATISTICI
# =============================================================================

print("\n=== STEP 1: Preparazione dati per test statistici ===")

# Carica dataset
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

# Usa preprocessing ottimale da analisi precedenti
def prepare_data():
    df_encoded = df_original.copy()
    
    # Demografia encoding
    df_encoded['eta_norm'] = (df_encoded['q1'] - 18) / (70 - 18)
    df_encoded['genere_donna'] = (df_encoded['q2'] == 'Donna').astype(int)
    df_encoded['titolo_magistrale'] = (df_encoded['q3'] == 'Laurea Magistrale').astype(int)
    df_encoded['titolo_postlaurea'] = (df_encoded['q3'] == 'Formazione Post Laurea (Master/Dottorato)').astype(int)
    df_encoded['occup_studente'] = (df_encoded['q4'] == 'Studente/ Studentessa').astype(int)
    df_encoded['occup_privato'] = (df_encoded['q4'] == 'Lavoro dipendente privato').astype(int)
    df_encoded['geo_nord'] = df_encoded['q5'].isin(['Nord Est', 'Nord Ovest']).astype(int)
    df_encoded['geo_centro'] = (df_encoded['q5'] == 'Centro').astype(int)
    df_encoded['reddito_alto'] = df_encoded['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
    df_encoded['reddito_basso'] = (df_encoded['q6'] == 'Meno di 15000').astype(int)
    
    demo_vars = ['eta_norm', 'genere_donna', 'titolo_magistrale', 'titolo_postlaurea', 
                 'occup_studente', 'occup_privato', 'geo_nord', 'geo_centro', 
                 'reddito_alto', 'reddito_basso']
    
    return df_encoded[demo_vars].fillna(df_encoded[demo_vars].mean())

data_demo = prepare_data()
scaler = RobustScaler()
data_scaled = scaler.fit_transform(data_demo)

print(f"Dataset preparato: {data_scaled.shape}")

# Clustering vincenti da analisi precedenti
winners = {
    'KMeans_3': KMeans(n_clusters=3, random_state=42, n_init=10),
    'Consensus_3': None,  # Implementeremo separatamente
    'Spectral_3': None    # Da ensemble precedente
}

# Fit clustering vincenti
clustering_results = {}

# KMeans_3
kmeans3 = KMeans(n_clusters=3, random_state=42, n_init=10)
labels_kmeans3 = kmeans3.fit_predict(data_scaled)
clustering_results['KMeans_3'] = {
    'labels': labels_kmeans3,
    'centers': kmeans3.cluster_centers_,
    'n_clusters': 3
}

# Hierarchical per confronto
hier3 = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels_hier3 = hier3.fit_predict(data_scaled)
clustering_results['Hierarchical_3'] = {
    'labels': labels_hier3,
    'centers': None,
    'n_clusters': 3
}

print("Clustering di riferimento completati")

# =============================================================================
# STEP 2: METRICHE ALTERNATIVE AVANZATE
# =============================================================================

print("\n=== STEP 2: Calcolo metriche alternative ===")

def calculate_advanced_metrics(data, labels, cluster_centers=None):
    """Calcola metriche clustering avanzate"""
    n_clusters = len(np.unique(labels))
    n_samples = len(data)
    
    metrics = {}
    
    # Silhouette Score (già usato)
    if n_clusters > 1:
        metrics['silhouette'] = silhouette_score(data, labels)
    else:
        metrics['silhouette'] = 0.0
    
    # Calinski-Harabasz Index (Variance Ratio Criterion)
    if n_clusters > 1:
        metrics['calinski_harabasz'] = calinski_harabasz_score(data, labels)
    else:
        metrics['calinski_harabasz'] = 0.0
    
    # Davies-Bouldin Index (lower is better)
    if n_clusters > 1:
        metrics['davies_bouldin'] = davies_bouldin_score(data, labels)
    else:
        metrics['davies_bouldin'] = np.inf
    
    # Within-Cluster Sum of Squares (WCSS)
    wcss = 0
    for i in range(n_clusters):
        cluster_data = data[labels == i]
        if len(cluster_data) > 0:
            if cluster_centers is not None:
                center = cluster_centers[i]
            else:
                center = np.mean(cluster_data, axis=0)
            wcss += np.sum((cluster_data - center) ** 2)
    metrics['wcss'] = wcss
    
    # Between-Cluster Sum of Squares (BCSS)
    overall_center = np.mean(data, axis=0)
    bcss = 0
    for i in range(n_clusters):
        cluster_size = np.sum(labels == i)
        if cluster_size > 0:
            if cluster_centers is not None:
                center = cluster_centers[i]
            else:
                center = np.mean(data[labels == i], axis=0)
            bcss += cluster_size * np.sum((center - overall_center) ** 2)
    metrics['bcss'] = bcss
    
    # Dunn Index (ratio of minimum inter-cluster distance to maximum intra-cluster distance)
    if n_clusters > 1:
        min_inter_cluster_dist = np.inf
        max_intra_cluster_dist = 0
        
        # Inter-cluster distances
        cluster_centers_calc = []
        for i in range(n_clusters):
            if cluster_centers is not None:
                cluster_centers_calc.append(cluster_centers[i])
            else:
                cluster_centers_calc.append(np.mean(data[labels == i], axis=0))
        
        for i in range(n_clusters):
            for j in range(i+1, n_clusters):
                dist = np.linalg.norm(cluster_centers_calc[i] - cluster_centers_calc[j])
                min_inter_cluster_dist = min(min_inter_cluster_dist, dist)
        
        # Intra-cluster distances
        for i in range(n_clusters):
            cluster_data = data[labels == i]
            if len(cluster_data) > 1:
                center = cluster_centers_calc[i]
                distances = [np.linalg.norm(point - center) for point in cluster_data]
                max_intra_cluster_dist = max(max_intra_cluster_dist, max(distances))
        
        if max_intra_cluster_dist > 0:
            metrics['dunn_index'] = min_inter_cluster_dist / max_intra_cluster_dist
        else:
            metrics['dunn_index'] = 0
    else:
        metrics['dunn_index'] = 0
    
    # Gap Statistic (approximation)
    # Confronta WCSS con WCSS di dati random
    n_refs = 10
    gap_wcss = []
    
    for _ in range(n_refs):
        # Genera dati random con stessa distribuzione
        random_data = np.random.uniform(
            low=np.min(data, axis=0), 
            high=np.max(data, axis=0), 
            size=data.shape
        )
        
        # Clustering sui dati random
        if cluster_centers is not None:
            random_kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            random_labels = random_kmeans.fit_predict(random_data)
            random_centers = random_kmeans.cluster_centers_
        else:
            random_hier = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
            random_labels = random_hier.fit_predict(random_data)
            random_centers = None
        
        # WCSS per dati random
        random_wcss = 0
        for i in range(n_clusters):
            cluster_data = random_data[random_labels == i]
            if len(cluster_data) > 0:
                if random_centers is not None:
                    center = random_centers[i]
                else:
                    center = np.mean(cluster_data, axis=0)
                random_wcss += np.sum((cluster_data - center) ** 2)
        
        gap_wcss.append(np.log(random_wcss))
    
    expected_log_wcss = np.mean(gap_wcss)
    metrics['gap_statistic'] = expected_log_wcss - np.log(wcss)
    
    return metrics

# Calcola metriche per tutti i clustering
print("Calcolando metriche avanzate...")

all_metrics = {}
for method_name, result in clustering_results.items():
    print(f"  {method_name}...")
    metrics = calculate_advanced_metrics(
        data_scaled, 
        result['labels'], 
        result['centers']
    )
    all_metrics[method_name] = metrics

# Tabella riassuntiva metriche
metrics_df = pd.DataFrame(all_metrics).T
print(f"\nMETRICHE ALTERNATIVE:")
print(metrics_df.round(3))

# =============================================================================
# STEP 3: TEST STATISTICI FORMALI
# =============================================================================

print(f"\n=== STEP 3: Test statistici formali ===")

# Focus su KMeans_3 (vincitore)
best_labels = clustering_results['KMeans_3']['labels']
n_clusters_best = 3

print(f"Testing KMeans_3 clustering (n_clusters={n_clusters_best})")

# 1. Test normalità per ogni cluster
print(f"\n1. TEST NORMALITÀ (Shapiro-Wilk per cluster):")
normality_results = {}

for cluster_id in range(n_clusters_best):
    cluster_data = data_scaled[best_labels == cluster_id]
    cluster_size = len(cluster_data)
    
    print(f"   Cluster {cluster_id} (n={cluster_size}):")
    
    if cluster_size >= 3:  # Minimo per Shapiro-Wilk
        # Test su prime 3 componenti principali per semplicità
        normality_cluster = {}
        
        for dim in range(min(3, data_scaled.shape[1])):
            if cluster_size <= 50:  # Shapiro-Wilk limite
                stat, p_value = shapiro(cluster_data[:, dim])
                normality_cluster[f'dim_{dim}'] = {'stat': stat, 'p_value': p_value}
                
                norm_status = "Normale" if p_value > 0.05 else "Non-normale"
                print(f"     Dim {dim}: W={stat:.3f}, p={p_value:.4f} ({norm_status})")
            else:
                print(f"     Dim {dim}: Cluster troppo grande per Shapiro-Wilk")
        
        normality_results[cluster_id] = normality_cluster
    else:
        print(f"     Cluster troppo piccolo per test normalità")

# 2. Test omogeneità varianze (Levene's test)
print(f"\n2. TEST OMOGENEITÀ VARIANZE (Levene):")

levene_results = {}
for dim in range(min(3, data_scaled.shape[1])):
    groups = [data_scaled[best_labels == i, dim] for i in range(n_clusters_best)]
    
    # Rimuovi gruppi vuoti
    groups_filtered = [g for g in groups if len(g) > 0]
    
    if len(groups_filtered) >= 2:
        stat, p_value = levene(*groups_filtered)
        levene_results[f'dim_{dim}'] = {'stat': stat, 'p_value': p_value}
        
        homog_status = "Omogenee" if p_value > 0.05 else "Non-omogenee"
        print(f"   Dim {dim}: W={stat:.3f}, p={p_value:.4f} ({homog_status})")

# 3. Test differenze tra cluster (Kruskal-Wallis - non parametrico)
print(f"\n3. TEST DIFFERENZE TRA CLUSTER (Kruskal-Wallis):")

kruskal_results = {}
significant_dims = []

for dim in range(data_scaled.shape[1]):
    groups = [data_scaled[best_labels == i, dim] for i in range(n_clusters_best)]
    groups_filtered = [g for g in groups if len(g) > 0]
    
    if len(groups_filtered) >= 2:
        stat, p_value = kruskal(*groups_filtered)
        kruskal_results[f'dim_{dim}'] = {'stat': stat, 'p_value': p_value}
        
        if p_value < 0.05:
            significant_dims.append(dim)

# Correzione Bonferroni per test multipli
if kruskal_results:
    p_values = [result['p_value'] for result in kruskal_results.values()]
    rejected, p_corrected, alpha_sidak, alpha_bonf = multipletests(
        p_values, alpha=0.05, method='bonferroni'
    )
    
    print(f"   Dimensioni significative (p < 0.05): {len(significant_dims)}/{data_scaled.shape[1]}")
    print(f"   Dimensioni significative post-Bonferroni: {np.sum(rejected)}/{data_scaled.shape[1]}")
    
    for i, (dim_key, result) in enumerate(kruskal_results.items()):
        dim_num = int(dim_key.split('_')[1])
        status = "***" if p_corrected[i] < 0.001 else "**" if p_corrected[i] < 0.01 else "*" if p_corrected[i] < 0.05 else "ns"
        print(f"     Dim {dim_num}: H={result['stat']:.3f}, p={result['p_value']:.4f}, p_corr={p_corrected[i]:.4f} {status}")

# 4. Permutation test per clustering significance
print(f"\n4. PERMUTATION TEST PER SIGNIFICATIVITÀ CLUSTERING:")

def clustering_score_func(X, y):
    """Funzione score per permutation test"""
    try:
        return silhouette_score(X, y)
    except:
        return 0.0

# Permutation test
n_permutations = 100  # Ridotto per velocità
print(f"   Eseguendo {n_permutations} permutazioni...")

# Score originale
original_score = silhouette_score(data_scaled, best_labels)

# Permutazione labels
np.random.seed(42)
permuted_scores = []

for i in range(n_permutations):
    permuted_labels = np.random.permutation(best_labels)
    
    # Verifica che ci siano ancora cluster multipli
    if len(np.unique(permuted_labels)) > 1:
        try:
            permuted_score = silhouette_score(data_scaled, permuted_labels)
            permuted_scores.append(permuted_score)
        except:
            permuted_scores.append(0.0)
    else:
        permuted_scores.append(0.0)

permuted_scores = np.array(permuted_scores)

# P-value empirico
p_value_perm = np.mean(permuted_scores >= original_score)

print(f"   Score originale: {original_score:.3f}")
print(f"   Score permutazioni: {np.mean(permuted_scores):.3f} ± {np.std(permuted_scores):.3f}")
print(f"   P-value empirico: {p_value_perm:.4f}")

if p_value_perm < 0.05:
    print(f"   ✓ CLUSTERING SIGNIFICATIVO (p < 0.05)")
else:
    print(f"   ⚠ CLUSTERING NON SIGNIFICATIVO (p ≥ 0.05)")

# =============================================================================
# STEP 4: STABILITY TESTING
# =============================================================================

print(f"\n=== STEP 4: Stability testing ===")

# Bootstrap stability (versione più rigorosa)
n_bootstrap = 50
bootstrap_scores = []
bootstrap_aris = []

print(f"   Bootstrap stability ({n_bootstrap} samples)...")

np.random.seed(42)
for i in range(n_bootstrap):
    # Bootstrap sample
    bootstrap_indices = np.random.choice(len(data_scaled), len(data_scaled), replace=True)
    bootstrap_data = data_scaled[bootstrap_indices]
    
    try:
        # Clustering su bootstrap
        bootstrap_kmeans = KMeans(n_clusters=3, random_state=42, n_init=5)
        bootstrap_labels = bootstrap_kmeans.fit_predict(bootstrap_data)
        
        # Silhouette su bootstrap
        if len(np.unique(bootstrap_labels)) > 1:
            bootstrap_score = silhouette_score(bootstrap_data, bootstrap_labels)
            bootstrap_scores.append(bootstrap_score)
            
            # ARI con clustering originale
            original_bootstrap = best_labels[bootstrap_indices]
            ari = adjusted_rand_score(bootstrap_labels, original_bootstrap)
            bootstrap_aris.append(ari)
        else:
            bootstrap_scores.append(0.0)
            bootstrap_aris.append(0.0)
            
    except:
        bootstrap_scores.append(0.0)
        bootstrap_aris.append(0.0)

bootstrap_scores = np.array(bootstrap_scores)
bootstrap_aris = np.array(bootstrap_aris)

# Statistiche stability
stability_silhouette = {
    'mean': np.mean(bootstrap_scores),
    'std': np.std(bootstrap_scores),
    'ci_lower': np.percentile(bootstrap_scores, 2.5),
    'ci_upper': np.percentile(bootstrap_scores, 97.5)
}

stability_ari = {
    'mean': np.mean(bootstrap_aris),
    'std': np.std(bootstrap_aris),
    'ci_lower': np.percentile(bootstrap_aris, 2.5),
    'ci_upper': np.percentile(bootstrap_aris, 97.5)
}

print(f"   Silhouette stability: {stability_silhouette['mean']:.3f} ± {stability_silhouette['std']:.3f}")
print(f"   Silhouette 95% CI: [{stability_silhouette['ci_lower']:.3f}, {stability_silhouette['ci_upper']:.3f}]")
print(f"   ARI stability: {stability_ari['mean']:.3f} ± {stability_ari['std']:.3f}")
print(f"   ARI 95% CI: [{stability_ari['ci_lower']:.3f}, {stability_ari['ci_upper']:.3f}]")

# Valutazione stability
if stability_ari['mean'] > 0.7:
    stability_assessment = "Alta stabilità"
elif stability_ari['mean'] > 0.5:
    stability_assessment = "Media stabilità"  
else:
    stability_assessment = "Bassa stabilità"

print(f"   Valutazione: {stability_assessment}")

# =============================================================================
# STEP 5: RIEPILOGO TEST STATISTICI
# =============================================================================

print(f"\n=== RIEPILOGO TEST STATISTICI ===")

print(f"METODO TESTATO: KMeans_3")
print(f"CAMPIONE: n={len(data_scaled)}, p={data_scaled.shape[1]} dimensioni")

print(f"\nMETRICHE PERFORMANCE:")
best_metrics = all_metrics['KMeans_3']
print(f"  Silhouette Score: {best_metrics['silhouette']:.3f}")
print(f"  Calinski-Harabasz: {best_metrics['calinski_harabasz']:.1f}")
print(f"  Davies-Bouldin: {best_metrics['davies_bouldin']:.3f} (lower better)")
print(f"  Dunn Index: {best_metrics['dunn_index']:.3f}")
print(f"  Gap Statistic: {best_metrics['gap_statistic']:.3f}")

print(f"\nSIGNIFICATIVITÀ STATISTICA:")
print(f"  Dimensioni discriminanti: {len(significant_dims)}/{data_scaled.shape[1]} ({len(significant_dims)/data_scaled.shape[1]*100:.1f}%)")
print(f"  Post-correzione Bonferroni: {np.sum(rejected) if 'rejected' in locals() else 'N/A'}")
print(f"  Permutation test p-value: {p_value_perm:.4f}")

significance_status = "SIGNIFICATIVO" if p_value_perm < 0.05 else "NON SIGNIFICATIVO"
print(f"  Clustering: {significance_status}")

print(f"\nSTABILITÀ:")
print(f"  Bootstrap ARI: {stability_ari['mean']:.3f} ± {stability_ari['std']:.3f}")
print(f"  Valutazione: {stability_assessment}")

print(f"\nVALIDITÀ ASSUNZIONI:")
normal_clusters = sum(1 for cluster_results in normality_results.values() 
                     if any(r['p_value'] > 0.05 for r in cluster_results.values()))
print(f"  Cluster con distribuzione normale: {normal_clusters}/{len(normality_results)}")

homogeneous_dims = sum(1 for result in levene_results.values() if result['p_value'] > 0.05)
print(f"  Dimensioni con varianze omogenee: {homogeneous_dims}/{len(levene_results)}")

print(f"\n=== CONCLUSIONE VALIDAZIONE STATISTICA ===")

if (p_value_perm < 0.05 and stability_ari['mean'] > 0.5 and 
    len(significant_dims)/data_scaled.shape[1] > 0.3):
    print("✓ CLUSTERING VALIDATO STATISTICAMENTE")
    print("  - Significatività confermata")
    print("  - Stabilità accettabile")
    print("  - Discriminazione dimensionale sufficiente")
elif p_value_perm < 0.05:
    print("⚠ CLUSTERING PARZIALMENTE VALIDATO")  
    print("  - Significativo ma stabilità/discriminazione limitate")
else:
    print("✗ CLUSTERING NON VALIDATO STATISTICAMENTE")
    print("  - Significatività non raggiunta")

print("\n=== MODULO TEST STATISTICI COMPLETATO ===")




# =============================================================================
# MODULO METRICHE ALTERNATIVE AVANZATE
# Gap Statistic, Stability Indices, Information-theoretic measures
# =============================================================================

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score
from sklearn.metrics.cluster import contingency_matrix
from scipy.spatial.distance import pdist, squareform
from scipy.stats import entropy
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO METRICHE ALTERNATIVE AVANZATE ===")

# =============================================================================
# STEP 1: PREPARAZIONE E CLUSTERING REFERENCE
# =============================================================================

print("\n=== STEP 1: Setup per metriche avanzate ===")

# Carica dataset ottimale
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

# Preprocessing ottimale
def prepare_data():
    df_encoded = df_original.copy()
    
    df_encoded['eta_norm'] = (df_encoded['q1'] - 18) / (70 - 18)
    df_encoded['genere_donna'] = (df_encoded['q2'] == 'Donna').astype(int)
    df_encoded['titolo_magistrale'] = (df_encoded['q3'] == 'Laurea Magistrale').astype(int)
    df_encoded['titolo_postlaurea'] = (df_encoded['q3'] == 'Formazione Post Laurea (Master/Dottorato)').astype(int)
    df_encoded['occup_studente'] = (df_encoded['q4'] == 'Studente/ Studentessa').astype(int)
    df_encoded['occup_privato'] = (df_encoded['q4'] == 'Lavoro dipendente privato').astype(int)
    df_encoded['geo_nord'] = df_encoded['q5'].isin(['Nord Est', 'Nord Ovest']).astype(int)
    df_encoded['geo_centro'] = (df_encoded['q5'] == 'Centro').astype(int)
    df_encoded['reddito_alto'] = df_encoded['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
    df_encoded['reddito_basso'] = (df_encoded['q6'] == 'Meno di 15000').astype(int)
    
    demo_vars = ['eta_norm', 'genere_donna', 'titolo_magistrale', 'titolo_postlaurea', 
                 'occup_studente', 'occup_privato', 'geo_nord', 'geo_centro', 
                 'reddito_alto', 'reddito_basso']
    
    return df_encoded[demo_vars].fillna(df_encoded[demo_vars].mean())

data_demo = prepare_data()
scaler = RobustScaler()
data_scaled = scaler.fit_transform(data_demo)

# Clustering vincente validato
kmeans3 = KMeans(n_clusters=3, random_state=42, n_init=10)
labels_best = kmeans3.fit_predict(data_scaled)

print(f"Dataset: {data_scaled.shape}")
print(f"Clustering: 3 cluster, distribuzione: {np.bincount(labels_best)}")

# =============================================================================
# STEP 2: GAP STATISTIC RIGOROSO
# =============================================================================

print("\n=== STEP 2: Gap Statistic rigoroso ===")

def calculate_gap_statistic(data, k_max=6, n_refs=20, random_state=42):
    """Calcola Gap Statistic rigoroso per selezione K ottimale"""
    
    n_samples, n_features = data.shape
    
    results = {
        'k': [],
        'log_wk': [],
        'expected_log_wk': [], 
        'gap': [],
        'gap_std': [],
        'gap_sk': []
    }
    
    np.random.seed(random_state)
    
    print("  Calcolando Gap Statistic...")
    
    for k in range(1, k_max + 1):
        print(f"    K = {k}...", end=" ")
        
        # WCSS per clustering reale
        if k == 1:
            # Un solo cluster = distanza da centroide generale
            center = np.mean(data, axis=0)
            wk = np.sum((data - center) ** 2)
        else:
            kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)
            labels = kmeans.fit_predict(data)
            
            wk = 0
            for i in range(k):
                cluster_data = data[labels == i]
                if len(cluster_data) > 0:
                    center = np.mean(cluster_data, axis=0)
                    wk += np.sum((cluster_data - center) ** 2)
        
        log_wk = np.log(wk)
        
        # WCSS per dati di riferimento (uniform random)
        data_min = np.min(data, axis=0)
        data_max = np.max(data, axis=0)
        
        ref_log_wks = []
        
        for ref in range(n_refs):
            # Genera dati uniformi nello spazio originale
            ref_data = np.random.uniform(
                low=data_min,
                high=data_max, 
                size=(n_samples, n_features)
            )
            
            # Clustering sui dati riferimento
            if k == 1:
                ref_center = np.mean(ref_data, axis=0)
                ref_wk = np.sum((ref_data - ref_center) ** 2)
            else:
                ref_kmeans = KMeans(n_clusters=k, random_state=ref, n_init=5)
                ref_labels = ref_kmeans.fit_predict(ref_data)
                
                ref_wk = 0
                for i in range(k):
                    ref_cluster = ref_data[ref_labels == i]
                    if len(ref_cluster) > 0:
                        ref_center = np.mean(ref_cluster, axis=0)
                        ref_wk += np.sum((ref_cluster - ref_center) ** 2)
            
            ref_log_wks.append(np.log(ref_wk))
        
        expected_log_wk = np.mean(ref_log_wks)
        gap = expected_log_wk - log_wk
        gap_std = np.std(ref_log_wks)
        
        # Gap statistic con standard error
        gap_sk = gap_std * np.sqrt(1 + 1/n_refs)
        
        results['k'].append(k)
        results['log_wk'].append(log_wk)
        results['expected_log_wk'].append(expected_log_wk)
        results['gap'].append(gap)
        results['gap_std'].append(gap_std)
        results['gap_sk'].append(gap_sk)
        
        print(f"Gap = {gap:.3f}")
    
    return results

# Calcola Gap Statistic
gap_results = calculate_gap_statistic(data_scaled, k_max=6, n_refs=25)

# Trova K ottimale con regola standard
gaps = np.array(gap_results['gap'])
gap_sks = np.array(gap_results['gap_sk'])

# Regola: primo K tale che Gap(k) >= Gap(k+1) - s_{k+1}
optimal_k_gap = 1
for k in range(len(gaps) - 1):
    if gaps[k] >= gaps[k+1] - gap_sks[k+1]:
        optimal_k_gap = k + 1
        break

print(f"\n  Gap Statistic K ottimale: {optimal_k_gap}")
print(f"  Gap values: {[f'{g:.3f}' for g in gaps]}")

# =============================================================================
# STEP 3: STABILITY INDICES AVANZATI
# =============================================================================

print("\n=== STEP 3: Stability indices avanzati ===")

def calculate_stability_indices(data, k, n_samples_range=[0.7, 0.8, 0.9], n_runs=30):
    """Calcola indici di stabilità avanzati"""
    
    n_total = len(data)
    stability_results = {}
    
    for subsample_ratio in n_samples_range:
        print(f"  Subsample ratio {subsample_ratio}...")
        
        n_subsample = int(subsample_ratio * n_total)
        aris = []
        jaccard_coeffs = []
        
        # Clustering di riferimento (full data)
        kmeans_ref = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels_ref = kmeans_ref.fit_predict(data)
        
        np.random.seed(42)
        
        for run in range(n_runs):
            # Subsample
            indices = np.random.choice(n_total, n_subsample, replace=False)
            data_sub = data[indices]
            labels_ref_sub = labels_ref[indices]
            
            try:
                # Clustering su subsample
                kmeans_sub = KMeans(n_clusters=k, random_state=run, n_init=5)
                labels_sub = kmeans_sub.fit_predict(data_sub)
                
                # ARI
                ari = adjusted_rand_score(labels_ref_sub, labels_sub)
                aris.append(ari)
                
                # Jaccard coefficient per ogni coppia di punti
                # Conta coppie che rimangono insieme/separate consistently
                n_sub = len(data_sub)
                same_cluster_ref = 0
                same_cluster_both = 0
                
                for i in range(n_sub):
                    for j in range(i+1, n_sub):
                        ref_same = (labels_ref_sub[i] == labels_ref_sub[j])
                        sub_same = (labels_sub[i] == labels_sub[j])
                        
                        if ref_same:
                            same_cluster_ref += 1
                            if sub_same:
                                same_cluster_both += 1
                
                if same_cluster_ref > 0:
                    jaccard = same_cluster_both / same_cluster_ref
                else:
                    jaccard = 1.0
                    
                jaccard_coeffs.append(jaccard)
                
            except Exception:
                aris.append(0.0)
                jaccard_coeffs.append(0.0)
        
        stability_results[subsample_ratio] = {
            'ari_mean': np.mean(aris),
            'ari_std': np.std(aris), 
            'jaccard_mean': np.mean(jaccard_coeffs),
            'jaccard_std': np.std(jaccard_coeffs),
            'n_runs': len(aris)
        }
    
    return stability_results

# Calcola stability indices per K=3
stability_indices = calculate_stability_indices(data_scaled, k=3)

print(f"  Risultati stabilità:")
for ratio, results in stability_indices.items():
    print(f"    Subsample {ratio*100:.0f}%: ARI={results['ari_mean']:.3f}±{results['ari_std']:.3f}, "
          f"Jaccard={results['jaccard_mean']:.3f}±{results['jaccard_std']:.3f}")

# =============================================================================
# STEP 4: INFORMATION-THEORETIC MEASURES
# =============================================================================

print("\n=== STEP 4: Information-theoretic measures ===")

def calculate_information_measures(data, labels):
    """Calcola metriche basate su teoria dell'informazione"""
    
    k = len(np.unique(labels))
    n = len(data)
    
    measures = {}
    
    # 1. Entropy del clustering
    cluster_counts = np.bincount(labels)
    cluster_probs = cluster_counts / n
    clustering_entropy = entropy(cluster_probs)
    measures['clustering_entropy'] = clustering_entropy
    
    # 2. Compattezza media intra-cluster (average intra-cluster distance)
    intra_cluster_dists = []
    
    for cluster_id in range(k):
        cluster_data = data[labels == cluster_id]
        if len(cluster_data) > 1:
            # Distanze pairwise nel cluster
            cluster_distances = pdist(cluster_data)
            intra_cluster_dists.extend(cluster_distances)
    
    avg_intra_dist = np.mean(intra_cluster_dists) if intra_cluster_dists else 0
    measures['avg_intra_cluster_distance'] = avg_intra_dist
    
    # 3. Separazione media inter-cluster (average inter-cluster distance)
    inter_cluster_dists = []
    
    # Centroidi cluster
    centroids = []
    for cluster_id in range(k):
        cluster_data = data[labels == cluster_id]
        if len(cluster_data) > 0:
            centroids.append(np.mean(cluster_data, axis=0))
    
    # Distanze tra centroidi
    if len(centroids) > 1:
        centroid_distances = pdist(centroids)
        avg_inter_dist = np.mean(centroid_distances)
    else:
        avg_inter_dist = 0
    
    measures['avg_inter_cluster_distance'] = avg_inter_dist
    
    # 4. Separation-to-Compactness Ratio
    if avg_intra_dist > 0:
        sep_comp_ratio = avg_inter_dist / avg_intra_dist
    else:
        sep_comp_ratio = np.inf
    
    measures['separation_compactness_ratio'] = sep_comp_ratio
    
    # 5. Effective Number of Clusters (basato su entropia)
    effective_k = np.exp(clustering_entropy)
    measures['effective_n_clusters'] = effective_k
    
    return measures

# Calcola information measures
info_measures = calculate_information_measures(data_scaled, labels_best)

print(f"  Information-theoretic measures:")
for measure, value in info_measures.items():
    if 'distance' in measure:
        print(f"    {measure}: {value:.3f}")
    elif measure == 'effective_n_clusters':
        print(f"    {measure}: {value:.2f}")
    else:
        print(f"    {measure}: {value:.3f}")

# =============================================================================
# STEP 5: CLUSTER QUALITY ASSESSMENT
# =============================================================================

print("\n=== STEP 5: Cluster quality assessment ===")

def assess_cluster_quality(data, labels):
    """Valutazione qualitativa dettagliata dei cluster"""
    
    k = len(np.unique(labels))
    n, p = data.shape
    
    quality_report = {}
    
    for cluster_id in range(k):
        cluster_data = data[labels == cluster_id]
        cluster_size = len(cluster_data)
        
        if cluster_size == 0:
            continue
        
        # Statistiche cluster
        cluster_center = np.mean(cluster_data, axis=0)
        cluster_std = np.std(cluster_data, axis=0)
        
        # Compattezza (varianza media)
        if cluster_size > 1:
            cluster_var = np.mean(np.var(cluster_data, axis=0))
            
            # Dispersione (max distanza dal centro)
            distances_to_center = np.linalg.norm(cluster_data - cluster_center, axis=1)
            max_distance = np.max(distances_to_center)
            avg_distance = np.mean(distances_to_center)
            
        else:
            cluster_var = 0
            max_distance = 0
            avg_distance = 0
        
        # Dimensioni più discriminanti
        overall_center = np.mean(data, axis=0)
        discrimination_scores = np.abs(cluster_center - overall_center)
        top_discriminating_dims = np.argsort(discrimination_scores)[-3:][::-1]
        
        quality_report[cluster_id] = {
            'size': cluster_size,
            'size_percentage': cluster_size / n * 100,
            'compactness_var': cluster_var,
            'max_distance_to_center': max_distance,
            'avg_distance_to_center': avg_distance,
            'top_discriminating_dimensions': top_discriminating_dims.tolist(),
            'discrimination_scores': discrimination_scores[top_discriminating_dims].tolist()
        }
    
    return quality_report

cluster_quality = assess_cluster_quality(data_scaled, labels_best)

print(f"  Cluster quality assessment:")
for cluster_id, quality in cluster_quality.items():
    print(f"    Cluster {cluster_id}:")
    print(f"      Size: {quality['size']} ({quality['size_percentage']:.1f}%)")
    print(f"      Compattezza (var): {quality['compactness_var']:.3f}")
    print(f"      Distanza media da centro: {quality['avg_distance_to_center']:.3f}")
    print(f"      Top dimensioni discriminanti: {quality['top_discriminating_dimensions']}")

# =============================================================================
# STEP 6: RIEPILOGO METRICHE AVANZATE
# =============================================================================

print(f"\n=== RIEPILOGO METRICHE AVANZATE ===")

print(f"CLUSTERING ANALIZZATO: KMeans K=3")
print(f"DATASET: n={data_scaled.shape[0]}, p={data_scaled.shape[1]}")

print(f"\nGAP STATISTIC:")
print(f"  K ottimale suggerito: {optimal_k_gap}")
print(f"  Gap(K=3): {gap_results['gap'][2]:.3f}")
max_gap_k = np.argmax(gap_results['gap']) + 1
print(f"  Gap massimo: K={max_gap_k} ({max(gap_results['gap']):.3f})")

print(f"\nSTABILITÀ:")
avg_stability = np.mean([r['ari_mean'] for r in stability_indices.values()])
print(f"  ARI medio subsample: {avg_stability:.3f}")

if avg_stability > 0.8:
    stability_level = "Molto Alta"
elif avg_stability > 0.7:
    stability_level = "Alta"
elif avg_stability > 0.5:
    stability_level = "Media"
else:
    stability_level = "Bassa"

print(f"  Livello stabilità: {stability_level}")

print(f"\nINFORMATION MEASURES:")
print(f"  Clustering entropy: {info_measures['clustering_entropy']:.3f}")
print(f"  Effective K: {info_measures['effective_n_clusters']:.2f}")
print(f"  Separation/Compactness: {info_measures['separation_compactness_ratio']:.3f}")

print(f"\nQUALITÀ CLUSTER:")
sizes = [q['size'] for q in cluster_quality.values()]
balance_cv = np.std(sizes) / np.mean(sizes)
print(f"  Bilanciamento (CV sizes): {balance_cv:.3f}")
print(f"  Distribuzione: {sizes}")

if balance_cv < 0.3:
    balance_quality = "Ben Bilanciati"
elif balance_cv < 0.6:
    balance_quality = "Moderatamente Bilanciati"
else:
    balance_quality = "Sbilanciati"

print(f"  Valutazione bilanciamento: {balance_quality}")

# Valutazione finale integrata
print(f"\n=== VALUTAZIONE FINALE INTEGRATA ===")

# Score composito
gap_score = 1.0 if optimal_k_gap == 3 else 0.5
stability_score = min(avg_stability, 1.0)
balance_score = 1.0 if balance_cv < 0.5 else 0.5
separation_score = min(info_measures['separation_compactness_ratio'] / 5.0, 1.0)

composite_score = (gap_score * 0.25 + stability_score * 0.4 + 
                  balance_score * 0.2 + separation_score * 0.15)

print(f"SCORE COMPOSITO: {composite_score:.3f}/1.000")

if composite_score > 0.8:
    final_assessment = "ECCELLENTE"
elif composite_score > 0.7:
    final_assessment = "BUONO"
elif composite_score > 0.6:
    final_assessment = "DISCRETO"
else:
    final_assessment = "MIGLIORABILE"

print(f"VALUTAZIONE COMPLESSIVA: {final_assessment}")

print(f"\nCONCLUSIONE:")
print(f"Il clustering KMeans K=3 con preprocessing RobustScaler")
print(f"su variabili demografiche mostra performance {final_assessment.lower()}")
print(f"con alta stabilità ({stability_level.lower()}) e buona separazione cluster.")

print("\n=== MODULO METRICHE ALTERNATIVE COMPLETATO ===")


