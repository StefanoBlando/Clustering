# =============================================================================
# MODULO 6: CORRESPONDENCE ANALYSIS (CA/MCA)
# Multiple Correspondence Analysis per variabili categoriali + Biplot + Clustering
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO 6: CORRESPONDENCE ANALYSIS (CA/MCA) ===")

# =============================================================================
# STEP 1: PREPARAZIONE DATI PER MCA
# =============================================================================

print("\n=== STEP 1: Preparazione dati categoriali per MCA ===")

# Carica dataset originale per variabili categoriali pure
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

print(f"Dataset originale: {df_original.shape}")

# Seleziona variabili categoriali principali per MCA
categorical_vars_mca = [
    'q2',   # Genere
    'q3',   # Titolo studio  
    'q4',   # Occupazione
    'q5',   # Geografia
    'q6',   # Reddito
    'q15',  # Frequenza comportamento 1
    'q17',  # Frequenza comportamento 2
    'q22',  # Trasporto (se disponibile)
    'q25',  # Ostacoli (se disponibile)
    'q26',  # Responsabilità (se disponibile)
    'q27',  # Frequenza comportamento 3
    'q28'   # Partecipazione (se disponibile)
]

# Filtra solo variabili esistenti
existing_vars = [var for var in categorical_vars_mca if var in df_original.columns]
print(f"Variabili categoriali disponibili: {len(existing_vars)}")
print("Lista:", existing_vars)

# Crea dataset MCA pulito
df_mca = df_original[existing_vars].copy()

# Gestisci missing values
print(f"\nMissing values per variabile:")
for var in existing_vars:
    missing_count = df_mca[var].isnull().sum()
    print(f"  {var}: {missing_count}")

# Riempi missing con "Missing" category
df_mca = df_mca.fillna('Missing')

# Mostra distribuzione variabili
print(f"\nDistribuzione categorie per variabile:")
for var in existing_vars:
    unique_vals = df_mca[var].nunique()
    sample_vals = list(df_mca[var].unique()[:5])
    print(f"  {var}: {unique_vals} categorie, Sample: {sample_vals}")

# =============================================================================
# STEP 2: IMPLEMENTAZIONE MCA (Multiple Correspondence Analysis)  
# =============================================================================

print(f"\n=== STEP 2: Multiple Correspondence Analysis ===")

# Implementazione MCA usando decomposizione SVD
def perform_mca(df_categorical, n_components=5):
    """
    Implementazione MCA semplificata usando one-hot encoding e SVD
    """
    # One-hot encoding di tutte le variabili categoriali
    df_encoded = pd.get_dummies(df_categorical, prefix_sep='_')
    
    # Matrice indicatori (Z)
    Z = df_encoded.values.astype(float)
    n, p = Z.shape
    
    # Calcola matrice corrispondenza
    # Masse righe (osservazioni)
    row_masses = np.ones(n) / n
    
    # Masse colonne (categorie) 
    col_masses = Z.sum(axis=0) / Z.sum()
    
    # Matrice residui standardizzati
    expected = np.outer(row_masses * n, col_masses * p)
    residuals = (Z - expected) / np.sqrt(expected)
    
    # SVD sui residuals
    U, s, Vt = np.linalg.svd(residuals, full_matrices=False)
    
    # Coordinate fattoriali
    # Righe (osservazioni)
    row_coords = U[:, :n_components] * s[:n_components]
    
    # Colonne (categorie)
    col_coords = Vt[:n_components, :].T
    
    # Inerzia (eigenvalues)
    eigenvalues = (s**2)[:n_components]
    explained_variance = eigenvalues / np.sum(s**2)
    
    return {
        'row_coordinates': row_coords,
        'column_coordinates': col_coords,
        'eigenvalues': eigenvalues,
        'explained_variance': explained_variance,
        'column_names': df_encoded.columns.tolist(),
        'singular_values': s[:n_components]
    }

# Esegui MCA
print("Eseguendo MCA...")
mca_results = perform_mca(df_mca, n_components=min(8, len(existing_vars)))

print(f"MCA completata:")
print(f"Osservazioni: {mca_results['row_coordinates'].shape[0]}")
print(f"Categorie totali: {len(mca_results['column_names'])}")
print(f"Componenti estratte: {mca_results['row_coordinates'].shape[1]}")

# Varianza spiegata
print(f"\nVarianza spiegata per componente:")
for i, var_exp in enumerate(mca_results['explained_variance']):
    print(f"  Dim {i+1}: {var_exp:.3f} ({var_exp*100:.1f}%)")

cumulative_var = np.cumsum(mca_results['explained_variance'])
print(f"Varianza cumulativa prime 3 dimensioni: {cumulative_var[2]:.3f} ({cumulative_var[2]*100:.1f}%)")

# =============================================================================
# STEP 3: BIPLOT INTERPRETAZIONE DIMENSIONI
# =============================================================================

print(f"\n=== STEP 3: Biplot e interpretazione dimensioni ===")

# Biplot: Osservazioni + Categorie
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

# Plot 1: Osservazioni nello spazio fattoriale
row_coords = mca_results['row_coordinates']
ax1.scatter(row_coords[:, 0], row_coords[:, 1], alpha=0.6, s=30, c='blue')
ax1.set_xlabel(f'Dim 1 ({mca_results["explained_variance"][0]*100:.1f}%)')
ax1.set_ylabel(f'Dim 2 ({mca_results["explained_variance"][1]*100:.1f}%)')
ax1.set_title('Osservazioni nello Spazio MCA')
ax1.grid(True, alpha=0.3)
ax1.axhline(y=0, color='red', linestyle='-', alpha=0.3)
ax1.axvline(x=0, color='red', linestyle='-', alpha=0.3)

# Plot 2: Categorie nello spazio fattoriale  
col_coords = mca_results['column_coordinates']
col_names = mca_results['column_names']

# Plot punti categorie
ax2.scatter(col_coords[:, 0], col_coords[:, 1], alpha=0.7, s=50, c='red')

# Aggiungi labels per categorie più importanti (contributo alto)
contributions = np.sqrt(col_coords[:, 0]**2 + col_coords[:, 1]**2)
top_contrib_idx = np.argsort(contributions)[-20:]  # Top 20 categorie

for idx in top_contrib_idx:
    ax2.annotate(col_names[idx], 
                (col_coords[idx, 0], col_coords[idx, 1]),
                xytext=(5, 5), textcoords='offset points',
                fontsize=8, alpha=0.8)

ax2.set_xlabel(f'Dim 1 ({mca_results["explained_variance"][0]*100:.1f}%)')
ax2.set_ylabel(f'Dim 2 ({mca_results["explained_variance"][1]*100:.1f}%)')
ax2.set_title('Categorie nello Spazio MCA (Top 20 contributori)')
ax2.grid(True, alpha=0.3)
ax2.axhline(y=0, color='red', linestyle='-', alpha=0.3)
ax2.axvline(x=0, color='red', linestyle='-', alpha=0.3)

plt.tight_layout()
plt.savefig('modulo6_mca_biplot.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# STEP 4: INTERPRETAZIONE DIMENSIONI MCA
# =============================================================================

print(f"\n=== STEP 4: Interpretazione dimensioni MCA ===")

# Analizza contributi delle categorie alle prime 2 dimensioni
def analyze_dimension_contributions(col_coords, col_names, dimension_idx, n_top=10):
    """Analizza i contributori principali a una dimensione MCA"""
    
    coords = col_coords[:, dimension_idx]
    abs_coords = np.abs(coords)
    
    # Top contributori positivi
    pos_idx = np.where(coords > 0)[0]
    pos_contrib = abs_coords[pos_idx]
    top_pos_idx = pos_idx[np.argsort(pos_contrib)[-n_top:]][::-1]
    
    # Top contributori negativi  
    neg_idx = np.where(coords < 0)[0]
    neg_contrib = abs_coords[neg_idx]
    top_neg_idx = neg_idx[np.argsort(neg_contrib)[-n_top:]][::-1]
    
    return top_pos_idx, top_neg_idx

# Dimensione 1
print("DIMENSIONE 1 - Contributori principali:")
pos_idx_1, neg_idx_1 = analyze_dimension_contributions(col_coords, col_names, 0)

print("  Polo positivo (destra):")
for idx in pos_idx_1:
    coord_val = col_coords[idx, 0]
    print(f"    {col_names[idx]}: {coord_val:.3f}")

print("  Polo negativo (sinistra):")
for idx in neg_idx_1:
    coord_val = col_coords[idx, 0]
    print(f"    {col_names[idx]}: {coord_val:.3f}")

# Dimensione 2
print("\nDIMENSIONE 2 - Contributori principali:")
pos_idx_2, neg_idx_2 = analyze_dimension_contributions(col_coords, col_names, 1)

print("  Polo positivo (alto):")
for idx in pos_idx_2:
    coord_val = col_coords[idx, 1]
    print(f"    {col_names[idx]}: {coord_val:.3f}")

print("  Polo negativo (basso):")
for idx in neg_idx_2:
    coord_val = col_coords[idx, 1]
    print(f"    {col_names[idx]}: {coord_val:.3f}")

# =============================================================================
# STEP 5: CLUSTERING NELLO SPAZIO FATTORIALE MCA
# =============================================================================

print(f"\n=== STEP 5: Clustering nello spazio fattoriale MCA ===")

# Usa prime 3 dimensioni MCA per clustering
mca_coords_3d = mca_results['row_coordinates'][:, :3]

# Test diversi valori di K
k_range_mca = range(2, 6)
mca_clustering_results = {
    'k': [],
    'silhouette': [],
    'inertia': []
}

print("Testing K-means nello spazio MCA:")

for k in k_range_mca:
    kmeans_mca = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels_mca = kmeans_mca.fit_predict(mca_coords_3d)
    
    sil_score = silhouette_score(mca_coords_3d, cluster_labels_mca)
    inertia = kmeans_mca.inertia_
    
    mca_clustering_results['k'].append(k)
    mca_clustering_results['silhouette'].append(sil_score)
    mca_clustering_results['inertia'].append(inertia)
    
    print(f"  K={k}: Silhouette={sil_score:.3f}, Inertia={inertia:.1f}")

# K ottimale per MCA
optimal_k_mca = mca_clustering_results['k'][np.argmax(mca_clustering_results['silhouette'])]
print(f"\nK ottimale MCA: {optimal_k_mca}")

# Clustering finale MCA
kmeans_mca_final = KMeans(n_clusters=optimal_k_mca, random_state=42, n_init=10)
mca_cluster_labels = kmeans_mca_final.fit_predict(mca_coords_3d)

# Distribuzione cluster MCA
mca_cluster_counts = pd.Series(mca_cluster_labels).value_counts().sort_index()
print(f"\nDistribuzione cluster MCA (K={optimal_k_mca}):")
for i, count in mca_cluster_counts.items():
    print(f"Cluster {i}: {count} osservazioni ({count/len(df_original)*100:.1f}%)")

# =============================================================================
# STEP 6: VISUALIZZAZIONE CLUSTERING MCA
# =============================================================================

print(f"\n=== STEP 6: Visualizzazione clustering MCA ===")

# Plot clustering nello spazio MCA
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

# Plot 1: Cluster nello spazio MCA (Dim 1 vs Dim 2)
colors = plt.cm.Set1(np.linspace(0, 1, optimal_k_mca))

for i in range(optimal_k_mca):
    mask = mca_cluster_labels == i
    ax1.scatter(mca_coords_3d[mask, 0], mca_coords_3d[mask, 1],
               c=[colors[i]], alpha=0.7, s=50,
               label=f'Cluster MCA {i} (n={mask.sum()})')

ax1.set_xlabel(f'Dim 1 MCA ({mca_results["explained_variance"][0]*100:.1f}%)')
ax1.set_ylabel(f'Dim 2 MCA ({mca_results["explained_variance"][1]*100:.1f}%)')
ax1.set_title('Clustering K-Means nello Spazio MCA')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.axhline(y=0, color='red', linestyle='-', alpha=0.3)
ax1.axvline(x=0, color='red', linestyle='-', alpha=0.3)

# Plot 2: Dim 1 vs Dim 3
for i in range(optimal_k_mca):
    mask = mca_cluster_labels == i
    ax2.scatter(mca_coords_3d[mask, 0], mca_coords_3d[mask, 2],
               c=[colors[i]], alpha=0.7, s=50,
               label=f'Cluster MCA {i} (n={mask.sum()})')

ax2.set_xlabel(f'Dim 1 MCA ({mca_results["explained_variance"][0]*100:.1f}%)')
ax2.set_ylabel(f'Dim 3 MCA ({mca_results["explained_variance"][2]*100:.1f}%)')
ax2.set_title('Clustering MCA - Dim 1 vs Dim 3')
ax2.legend()
ax2.grid(True, alpha=0.3)
ax2.axhline(y=0, color='red', linestyle='-', alpha=0.3)
ax2.axvline(x=0, color='red', linestyle='-', alpha=0.3)

plt.tight_layout()
plt.savefig('modulo6_mca_clustering.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# STEP 7: PROFILING CLUSTER MCA TRAMITE CONTRIBUTI
# =============================================================================

print(f"\n=== STEP 7: Profiling cluster MCA ===")

# Crea dataframe con risultati MCA
df_mca_results = df_original.copy()
df_mca_results['mca_cluster'] = mca_cluster_labels
df_mca_results['mca_dim1'] = mca_coords_3d[:, 0]
df_mca_results['mca_dim2'] = mca_coords_3d[:, 1] 
df_mca_results['mca_dim3'] = mca_coords_3d[:, 2]

# Profiling per ogni cluster MCA
for cluster_id in range(optimal_k_mca):
    cluster_data = df_mca_results[df_mca_results['mca_cluster'] == cluster_id]
    n_obs = len(cluster_data)
    
    print(f"\n--- CLUSTER MCA {cluster_id} ({n_obs} osservazioni, {n_obs/len(df_original)*100:.1f}%) ---")
    
    # Coordinate medie nel spazio MCA
    mean_dim1 = cluster_data['mca_dim1'].mean()
    mean_dim2 = cluster_data['mca_dim2'].mean()
    mean_dim3 = cluster_data['mca_dim3'].mean()
    print(f"Coordinate medie MCA: Dim1={mean_dim1:.2f}, Dim2={mean_dim2:.2f}, Dim3={mean_dim3:.2f}")
    
    # Profilo variabili categoriali principali
    print("Profilo categoriale:")
    for var in existing_vars[:6]:  # Prime 6 variabili più importanti
        if var in cluster_data.columns:
            mode_cat = cluster_data[var].mode()
            if len(mode_cat) > 0:
                freq = (cluster_data[var] == mode_cat.iloc[0]).sum()
                pct = freq / len(cluster_data) * 100
                print(f"  {var}: {mode_cat.iloc[0]} ({pct:.0f}%)")

# =============================================================================
# STEP 8: CONFRONTO MCA CON ALTRI CLUSTERING
# =============================================================================

print(f"\n=== STEP 8: Confronto MCA con clustering precedenti ===")

# Confronta con risultati Hierarchical (Modulo 5)
try:
    hier_results = pd.read_csv('modulo5_hierarchical_results.csv')
    if len(hier_results) == len(df_mca_results):
        from sklearn.metrics import adjusted_rand_score
        
        ari_mca_hier = adjusted_rand_score(mca_cluster_labels, hier_results['hier_cluster'])
        print(f"ARI MCA vs Hierarchical: {ari_mca_hier:.3f}")
        
        # Tabella crosstab
        print(f"\nTabella confronto MCA vs Hierarchical:")
        crosstab_mca_hier = pd.crosstab(mca_cluster_labels, hier_results['hier_cluster'],
                                       margins=True, margins_name="Totale")
        print(crosstab_mca_hier)
        
    else:
        print("Dimensioni diverse - confronto non possibile")
        
except FileNotFoundError:
    print("Risultati Hierarchical non trovati")

# Silhouette comparison
mca_silhouette = silhouette_score(mca_coords_3d, mca_cluster_labels)
print(f"\nQualità clustering MCA:")
print(f"Silhouette MCA: {mca_silhouette:.3f}")

print("=== Continua con STEP 9: Export e Report finale ===")
