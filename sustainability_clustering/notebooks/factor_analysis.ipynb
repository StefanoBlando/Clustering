# =============================================================================
# MODULO 8: ANALISI FATTORIALE + CLUSTERING SUI FATTORI
# Opzione D: FA prima, poi selezione fattori significativi, poi clustering
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import FactorAnalysis, PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score
from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO 8: ANALISI FATTORIALE + CLUSTERING ===")

# =============================================================================
# STEP 1: DATASET LIKERT PURO
# =============================================================================

print("\n=== STEP 1: Preparazione dataset Likert puro ===")

# Carica dataset originale
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

# Variabili Likert 1-7 (attitudinali pure)
likert_vars = ['q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 
               'q16', 'q18', 'q19', 'q20', 'q21', 'q23', 'q24', 'q29']

# Filtra variabili esistenti
existing_likert = [var for var in likert_vars if var in df_original.columns]
print(f"Variabili Likert disponibili: {len(existing_likert)}")

# Dataset Likert puro
df_likert = df_original[existing_likert].copy()

# Gestisci missing values
df_likert = df_likert.fillna(df_likert.mean())

print(f"Dataset Likert: {df_likert.shape}")
print(f"Missing values: {df_likert.isnull().sum().sum()}")

# Statistiche descrittive
print(f"\nStatistiche Likert (1-7):")
print(df_likert.describe().round(2))

# =============================================================================
# STEP 2: TEST FATTORABILITÀ
# =============================================================================

print(f"\n=== STEP 2: Test di fattorabilità ===")

# Standardizzazione per test
scaler = StandardScaler()
X_likert_scaled = scaler.fit_transform(df_likert)

# Test di Bartlett (H0: matrice correlazione = identità)
chi_square_value, p_value = calculate_bartlett_sphericity(df_likert)
print(f"Test di Bartlett:")
print(f"  Chi-square: {chi_square_value:.2f}")
print(f"  p-value: {p_value:.6f}")
if p_value < 0.05:
    print("  ✓ Rifiuta H0: i dati sono fattorizzabili")
else:
    print("  ✗ Non rifiuta H0: i dati potrebbero non essere fattorizzabili")

# Kaiser-Meyer-Olkin (KMO) test
kmo_all, kmo_model = calculate_kmo(df_likert)
print(f"\nKaiser-Meyer-Olkin (KMO):")
print(f"  KMO complessivo: {kmo_model:.3f}")
if kmo_model > 0.8:
    print("  ✓ Eccellente per analisi fattoriale")
elif kmo_model > 0.7:
    print("  ✓ Buono per analisi fattoriale")  
elif kmo_model > 0.6:
    print("  ✓ Mediocre ma accettabile")
else:
    print("  ✗ Non adatto per analisi fattoriale")

# =============================================================================
# STEP 3: DETERMINAZIONE NUMERO FATTORI
# =============================================================================

print(f"\n=== STEP 3: Determinazione numero fattori ===")

# Criterio Kaiser (eigenvalues > 1)
from sklearn.decomposition import PCA
pca_eigenvals = PCA().fit(X_likert_scaled)
eigenvalues = pca_eigenvals.explained_variance_

kaiser_factors = np.sum(eigenvalues > 1)
print(f"Criterio Kaiser (eigenvalue > 1): {kaiser_factors} fattori")

# Scree plot
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(eigenvalues)+1), eigenvalues, 'bo-', linewidth=2, markersize=8)
plt.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Kaiser criterion')
plt.xlabel('Fattore')
plt.ylabel('Eigenvalue')
plt.title('Scree Plot - Determinazione Numero Fattori')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.savefig('modulo8_scree_plot.png', dpi=300, bbox_inches='tight')
plt.show()

# Varianza spiegata cumulativa
cum_var_explained = np.cumsum(pca_eigenvals.explained_variance_ratio_)
print(f"\nVarianza spiegata cumulativa:")
for i in range(min(8, len(cum_var_explained))):
    print(f"  {i+1} fattori: {cum_var_explained[i]*100:.1f}%")

# Criterio 70% varianza
factors_70pct = np.argmax(cum_var_explained >= 0.70) + 1
print(f"Fattori per 70% varianza: {factors_70pct}")

# Test scree (gomito visuale)
print(f"Esame visuale scree plot per determinare gomito...")

# Range di fattori da testare
n_factors_range = range(2, min(8, len(existing_likert)//2))
print(f"Testing {min(n_factors_range)}-{max(n_factors_range)} fattori")

# =============================================================================
# STEP 4: ANALISI FATTORIALE CON DIVERSI N
# =============================================================================

print(f"\n=== STEP 4: Analisi fattoriale con diversi N ===")

fa_results = []

for n_factors in n_factors_range:
    print(f"\nTesting {n_factors} fattori...")
    
    try:
        # Factor Analysis
        fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax', method='minres')
        fa.fit(df_likert)
        
        # Metriche
        eigenvals_fa = fa.get_eigenvalues()[0]
        communalities = fa.get_communalities()
        factor_loadings = fa.loadings_
        
        # Varianza spiegata
        var_explained = np.sum(eigenvals_fa) / len(existing_likert)
        
        # Interpretabilità (loadings forti)
        strong_loadings = np.sum(np.abs(factor_loadings) > 0.5)
        
        fa_results.append({
            'n_factors': n_factors,
            'variance_explained': var_explained,
            'mean_communality': np.mean(communalities),
            'strong_loadings': strong_loadings,
            'eigenvals': eigenvals_fa
        })
        
        print(f"  Varianza spiegata: {var_explained*100:.1f}%")
        print(f"  Communalities medie: {np.mean(communalities):.3f}")
        print(f"  Loadings forti (>0.5): {strong_loadings}")
        
    except Exception as e:
        print(f"  Errore: {e}")

# Selezione numero fattori ottimale
fa_df = pd.DataFrame(fa_results)

# Criterio composto: 60%+ varianza + communalities decenti
acceptable_fa = fa_df[
    (fa_df['variance_explained'] > 0.6) & 
    (fa_df['mean_communality'] > 0.4)
]

if len(acceptable_fa) > 0:
    # Prendi il più semplice tra quelli accettabili
    optimal_n_factors = acceptable_fa['n_factors'].min()
else:
    # Fallback al criterio Kaiser
    optimal_n_factors = kaiser_factors

print(f"\nNumero fattori ottimale scelto: {optimal_n_factors}")

# =============================================================================
# STEP 5: ANALISI FATTORIALE FINALE
# =============================================================================

print(f"\n=== STEP 5: Analisi fattoriale finale con {optimal_n_factors} fattori ===")

# Fit modello finale
fa_final = FactorAnalyzer(n_factors=optimal_n_factors, rotation='varimax', method='minres')
fa_final.fit(df_likert)

# Risultati
loadings = fa_final.loadings_
communalities = fa_final.get_communalities()
eigenvals_final = fa_final.get_eigenvalues()[0]
factor_names = [f'Factor_{i+1}' for i in range(optimal_n_factors)]

print(f"Analisi completata:")
print(f"  Varianza totale spiegata: {np.sum(eigenvals_final)/len(existing_likert)*100:.1f}%")
print(f"  Communalities: {np.mean(communalities):.3f} ± {np.std(communalities):.3f}")

# Matrice loadings
loadings_df = pd.DataFrame(loadings, columns=factor_names, index=existing_likert)
print(f"\nMatrix loadings (|loading| > 0.4):")
for factor in factor_names:
    print(f"\n{factor}:")
    strong_vars = loadings_df[loadings_df[factor].abs() > 0.4][factor].sort_values(key=abs, ascending=False)
    for var, loading in strong_vars.items():
        print(f"  {var}: {loading:.3f}")

# =============================================================================
# STEP 6: INTERPRETAZIONE FATTORI
# =============================================================================

print(f"\n=== STEP 6: Interpretazione fattori ===")

factor_interpretations = {}

for i, factor in enumerate(factor_names):
    print(f"\n{factor}:")
    
    # Variabili con loading alto
    high_loadings = loadings_df[loadings_df[factor].abs() > 0.4][factor]
    high_loadings_sorted = high_loadings.sort_values(key=abs, ascending=False)
    
    if len(high_loadings_sorted) > 0:
        print("Variabili principali:")
        for var, loading in high_loadings_sorted.items():
            print(f"  {var}: {loading:.3f}")
        
        # Tentativo interpretazione semantica
        # Guardando le domande Q7-Q29 per interpretazione
        top_vars = high_loadings_sorted.head(3).index.tolist()
        
        # Logica interpretativa semplificata
        if any('q8' in var or 'q13' in var or 'q19' in var for var in top_vars):
            interpretation = "Atteggiamenti Pro-Ambiente"
        elif any('q20' in var or 'q21' in var or 'q23' in var for var in top_vars):
            interpretation = "Comportamenti Sostenibili"  
        elif any('q7' in var or 'q9' in var or 'q10' in var for var in top_vars):
            interpretation = "Consapevolezza Ambientale"
        else:
            interpretation = f"Fattore Misto {i+1}"
            
        factor_interpretations[factor] = interpretation
        print(f"Interpretazione: {interpretation}")
    else:
        factor_interpretations[factor] = f"Fattore Debole {i+1}"
        print("Nessun loading significativo")

# =============================================================================
# STEP 7: SCORES FATTORIALI E CLUSTERING
# =============================================================================

print(f"\n=== STEP 7: Calcolo factor scores e clustering ===")

# Calcola factor scores
factor_scores = fa_final.transform(df_likert)
factor_scores_df = pd.DataFrame(factor_scores, columns=factor_names)

print(f"Factor scores calcolati: {factor_scores_df.shape}")
print(f"Statistiche factor scores:")
print(factor_scores_df.describe().round(3))

# Standardizza factor scores per clustering
scaler_factors = StandardScaler()
factor_scores_scaled = scaler_factors.fit_transform(factor_scores)

# Test diversi K per clustering sui fattori
k_range = range(2, 7)
clustering_results = {
    'k': [],
    'silhouette': [],
    'inertia': []
}

print(f"\nTesting clustering sui {optimal_n_factors} fattori:")

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(factor_scores_scaled)
    
    n_unique = len(np.unique(cluster_labels))
    if n_unique == k:
        sil = silhouette_score(factor_scores_scaled, cluster_labels)
        inertia = kmeans.inertia_
        
        clustering_results['k'].append(k)
        clustering_results['silhouette'].append(sil)
        clustering_results['inertia'].append(inertia)
        
        print(f"  K={k}: Silhouette={sil:.3f}, Inertia={inertia:.1f}")
    else:
        print(f"  K={k}: Solo {n_unique} cluster effettivi")

# Seleziona K ottimale
if clustering_results['silhouette']:
    optimal_k_factors = clustering_results['k'][np.argmax(clustering_results['silhouette'])]
    best_silhouette = max(clustering_results['silhouette'])
    print(f"\nK ottimale per fattori: {optimal_k_factors} (Silhouette: {best_silhouette:.3f})")
else:
    optimal_k_factors = 3
    print(f"\nFallback K=3")

print("=== Continua con clustering finale e confronti ===")
