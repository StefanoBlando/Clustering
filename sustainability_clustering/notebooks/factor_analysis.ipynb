# =============================================================================
# MODULO 8: ANALISI FATTORIALE + CLUSTERING SUI FATTORI
# Opzione D: FA prima, poi selezione fattori significativi, poi clustering
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import FactorAnalysis, PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score
from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO 8: ANALISI FATTORIALE + CLUSTERING ===")

# =============================================================================
# STEP 1: DATASET LIKERT PURO
# =============================================================================

print("\n=== STEP 1: Preparazione dataset Likert puro ===")

# Carica dataset originale
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

# Variabili Likert 1-7 (attitudinali pure)
likert_vars = ['q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 
               'q16', 'q18', 'q19', 'q20', 'q21', 'q23', 'q24', 'q29']

# Filtra variabili esistenti
existing_likert = [var for var in likert_vars if var in df_original.columns]
print(f"Variabili Likert disponibili: {len(existing_likert)}")

# Dataset Likert puro
df_likert = df_original[existing_likert].copy()

# Gestisci missing values
df_likert = df_likert.fillna(df_likert.mean())

print(f"Dataset Likert: {df_likert.shape}")
print(f"Missing values: {df_likert.isnull().sum().sum()}")

# Statistiche descrittive
print(f"\nStatistiche Likert (1-7):")
print(df_likert.describe().round(2))

# =============================================================================
# STEP 2: TEST FATTORABILITÀ
# =============================================================================

print(f"\n=== STEP 2: Test di fattorabilità ===")

# Standardizzazione per test
scaler = StandardScaler()
X_likert_scaled = scaler.fit_transform(df_likert)

# Test di Bartlett (H0: matrice correlazione = identità)
chi_square_value, p_value = calculate_bartlett_sphericity(df_likert)
print(f"Test di Bartlett:")
print(f"  Chi-square: {chi_square_value:.2f}")
print(f"  p-value: {p_value:.6f}")
if p_value < 0.05:
    print("  ✓ Rifiuta H0: i dati sono fattorizzabili")
else:
    print("  ✗ Non rifiuta H0: i dati potrebbero non essere fattorizzabili")

# Kaiser-Meyer-Olkin (KMO) test
kmo_all, kmo_model = calculate_kmo(df_likert)
print(f"\nKaiser-Meyer-Olkin (KMO):")
print(f"  KMO complessivo: {kmo_model:.3f}")
if kmo_model > 0.8:
    print("  ✓ Eccellente per analisi fattoriale")
elif kmo_model > 0.7:
    print("  ✓ Buono per analisi fattoriale")  
elif kmo_model > 0.6:
    print("  ✓ Mediocre ma accettabile")
else:
    print("  ✗ Non adatto per analisi fattoriale")

# =============================================================================
# STEP 3: DETERMINAZIONE NUMERO FATTORI
# =============================================================================

print(f"\n=== STEP 3: Determinazione numero fattori ===")

# Criterio Kaiser (eigenvalues > 1)
from sklearn.decomposition import PCA
pca_eigenvals = PCA().fit(X_likert_scaled)
eigenvalues = pca_eigenvals.explained_variance_

kaiser_factors = np.sum(eigenvalues > 1)
print(f"Criterio Kaiser (eigenvalue > 1): {kaiser_factors} fattori")

# Scree plot
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(eigenvalues)+1), eigenvalues, 'bo-', linewidth=2, markersize=8)
plt.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Kaiser criterion')
plt.xlabel('Fattore')
plt.ylabel('Eigenvalue')
plt.title('Scree Plot - Determinazione Numero Fattori')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.savefig('modulo8_scree_plot.png', dpi=300, bbox_inches='tight')
plt.show()

# Varianza spiegata cumulativa
cum_var_explained = np.cumsum(pca_eigenvals.explained_variance_ratio_)
print(f"\nVarianza spiegata cumulativa:")
for i in range(min(8, len(cum_var_explained))):
    print(f"  {i+1} fattori: {cum_var_explained[i]*100:.1f}%")

# Criterio 70% varianza
factors_70pct = np.argmax(cum_var_explained >= 0.70) + 1
print(f"Fattori per 70% varianza: {factors_70pct}")

# Test scree (gomito visuale)
print(f"Esame visuale scree plot per determinare gomito...")

# Range di fattori da testare
n_factors_range = range(2, min(8, len(existing_likert)//2))
print(f"Testing {min(n_factors_range)}-{max(n_factors_range)} fattori")

# =============================================================================
# STEP 4: ANALISI FATTORIALE CON DIVERSI N
# =============================================================================

print(f"\n=== STEP 4: Analisi fattoriale con diversi N ===")

fa_results = []

for n_factors in n_factors_range:
    print(f"\nTesting {n_factors} fattori...")
    
    try:
        # Factor Analysis
        fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax', method='minres')
        fa.fit(df_likert)
        
        # Metriche
        eigenvals_fa = fa.get_eigenvalues()[0]
        communalities = fa.get_communalities()
        factor_loadings = fa.loadings_
        
        # Varianza spiegata
        var_explained = np.sum(eigenvals_fa) / len(existing_likert)
        
        # Interpretabilità (loadings forti)
        strong_loadings = np.sum(np.abs(factor_loadings) > 0.5)
        
        fa_results.append({
            'n_factors': n_factors,
            'variance_explained': var_explained,
            'mean_communality': np.mean(communalities),
            'strong_loadings': strong_loadings,
            'eigenvals': eigenvals_fa
        })
        
        print(f"  Varianza spiegata: {var_explained*100:.1f}%")
        print(f"  Communalities medie: {np.mean(communalities):.3f}")
        print(f"  Loadings forti (>0.5): {strong_loadings}")
        
    except Exception as e:
        print(f"  Errore: {e}")

# Selezione numero fattori ottimale
fa_df = pd.DataFrame(fa_results)

# Criterio composto: 60%+ varianza + communalities decenti
acceptable_fa = fa_df[
    (fa_df['variance_explained'] > 0.6) & 
    (fa_df['mean_communality'] > 0.4)
]

if len(acceptable_fa) > 0:
    # Prendi il più semplice tra quelli accettabili
    optimal_n_factors = acceptable_fa['n_factors'].min()
else:
    # Fallback al criterio Kaiser
    optimal_n_factors = kaiser_factors

print(f"\nNumero fattori ottimale scelto: {optimal_n_factors}")

# =============================================================================
# STEP 5: ANALISI FATTORIALE FINALE
# =============================================================================

print(f"\n=== STEP 5: Analisi fattoriale finale con {optimal_n_factors} fattori ===")

# Fit modello finale
fa_final = FactorAnalyzer(n_factors=optimal_n_factors, rotation='varimax', method='minres')
fa_final.fit(df_likert)

# Risultati
loadings = fa_final.loadings_
communalities = fa_final.get_communalities()
eigenvals_final = fa_final.get_eigenvalues()[0]
factor_names = [f'Factor_{i+1}' for i in range(optimal_n_factors)]

print(f"Analisi completata:")
print(f"  Varianza totale spiegata: {np.sum(eigenvals_final)/len(existing_likert)*100:.1f}%")
print(f"  Communalities: {np.mean(communalities):.3f} ± {np.std(communalities):.3f}")

# Matrice loadings
loadings_df = pd.DataFrame(loadings, columns=factor_names, index=existing_likert)
print(f"\nMatrix loadings (|loading| > 0.4):")
for factor in factor_names:
    print(f"\n{factor}:")
    strong_vars = loadings_df[loadings_df[factor].abs() > 0.4][factor].sort_values(key=abs, ascending=False)
    for var, loading in strong_vars.items():
        print(f"  {var}: {loading:.3f}")

# =============================================================================
# STEP 6: INTERPRETAZIONE FATTORI
# =============================================================================

print(f"\n=== STEP 6: Interpretazione fattori ===")

factor_interpretations = {}

for i, factor in enumerate(factor_names):
    print(f"\n{factor}:")
    
    # Variabili con loading alto
    high_loadings = loadings_df[loadings_df[factor].abs() > 0.4][factor]
    high_loadings_sorted = high_loadings.sort_values(key=abs, ascending=False)
    
    if len(high_loadings_sorted) > 0:
        print("Variabili principali:")
        for var, loading in high_loadings_sorted.items():
            print(f"  {var}: {loading:.3f}")
        
        # Tentativo interpretazione semantica
        # Guardando le domande Q7-Q29 per interpretazione
        top_vars = high_loadings_sorted.head(3).index.tolist()
        
        # Logica interpretativa semplificata
        if any('q8' in var or 'q13' in var or 'q19' in var for var in top_vars):
            interpretation = "Atteggiamenti Pro-Ambiente"
        elif any('q20' in var or 'q21' in var or 'q23' in var for var in top_vars):
            interpretation = "Comportamenti Sostenibili"  
        elif any('q7' in var or 'q9' in var or 'q10' in var for var in top_vars):
            interpretation = "Consapevolezza Ambientale"
        else:
            interpretation = f"Fattore Misto {i+1}"
            
        factor_interpretations[factor] = interpretation
        print(f"Interpretazione: {interpretation}")
    else:
        factor_interpretations[factor] = f"Fattore Debole {i+1}"
        print("Nessun loading significativo")

# =============================================================================
# STEP 7: SCORES FATTORIALI E CLUSTERING
# =============================================================================

print(f"\n=== STEP 7: Calcolo factor scores e clustering ===")

# Calcola factor scores
factor_scores = fa_final.transform(df_likert)
factor_scores_df = pd.DataFrame(factor_scores, columns=factor_names)

print(f"Factor scores calcolati: {factor_scores_df.shape}")
print(f"Statistiche factor scores:")
print(factor_scores_df.describe().round(3))

# Standardizza factor scores per clustering
scaler_factors = StandardScaler()
factor_scores_scaled = scaler_factors.fit_transform(factor_scores)

# Test diversi K per clustering sui fattori
k_range = range(2, 7)
clustering_results = {
    'k': [],
    'silhouette': [],
    'inertia': []
}

print(f"\nTesting clustering sui {optimal_n_factors} fattori:")

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(factor_scores_scaled)
    
    n_unique = len(np.unique(cluster_labels))
    if n_unique == k:
        sil = silhouette_score(factor_scores_scaled, cluster_labels)
        inertia = kmeans.inertia_
        
        clustering_results['k'].append(k)
        clustering_results['silhouette'].append(sil)
        clustering_results['inertia'].append(inertia)
        
        print(f"  K={k}: Silhouette={sil:.3f}, Inertia={inertia:.1f}")
    else:
        print(f"  K={k}: Solo {n_unique} cluster effettivi")

# Seleziona K ottimale
if clustering_results['silhouette']:
    optimal_k_factors = clustering_results['k'][np.argmax(clustering_results['silhouette'])]
    best_silhouette = max(clustering_results['silhouette'])
    print(f"\nK ottimale per fattori: {optimal_k_factors} (Silhouette: {best_silhouette:.3f})")
else:
    optimal_k_factors = 3
    print(f"\nFallback K=3")

print("=== Continua con clustering finale e confronti ===")



# =============================================================================
# STEP 8: CLUSTERING FINALE SUI FATTORI
# =============================================================================

print(f"\n=== STEP 8: Clustering finale con K={optimal_k_factors} ===")

# Clustering finale
kmeans_factors = KMeans(n_clusters=optimal_k_factors, random_state=42, n_init=10)
factor_cluster_labels = kmeans_factors.fit_predict(factor_scores_scaled)

# Distribuzione cluster
print(f"Distribuzione cluster sui fattori:")
factor_cluster_counts = pd.Series(factor_cluster_labels).value_counts().sort_index()
for i, count in factor_cluster_counts.items():
    pct = count / len(factor_cluster_labels) * 100
    print(f"Cluster {i}: {count} obs ({pct:.1f}%)")

# Qualità finale
final_silhouette = silhouette_score(factor_scores_scaled, factor_cluster_labels)
print(f"Silhouette finale: {final_silhouette:.3f}")

# =============================================================================
# STEP 9: PROFILING CLUSTER SUI FATTORI
# =============================================================================

print(f"\n=== STEP 9: Profiling cluster sui fattori ===")

# Dataset con risultati
df_factor_results = df_original.copy()
df_factor_results['factor_cluster'] = factor_cluster_labels

# Aggiungi factor scores per interpretazione
for i, factor_name in enumerate(factor_names):
    df_factor_results[factor_name] = factor_scores[:, i]

# Profiling per ogni cluster
cluster_interpretations = {}

for cluster_id in range(optimal_k_factors):
    cluster_data = df_factor_results[df_factor_results['factor_cluster'] == cluster_id]
    n_obs = len(cluster_data)
    
    print(f"\n--- CLUSTER FATTORIALE {cluster_id} ({n_obs} obs, {n_obs/len(df_factor_results)*100:.1f}%) ---")
    
    # Profile sui fattori
    factor_profile = []
    for factor_name in factor_names:
        mean_score = cluster_data[factor_name].mean()
        factor_profile.append(mean_score)
        interpretation = factor_interpretations[factor_name]
        
        level = "Alto" if mean_score > 0.5 else "Basso" if mean_score < -0.5 else "Medio"
        print(f"{factor_name} ({interpretation}): {mean_score:.2f} [{level}]")
    
    # Demografia di supporto (per context)
    if 'q2' in cluster_data.columns:
        genere_dist = cluster_data['q2'].value_counts()
        main_gender = genere_dist.index[0]
        gender_pct = genere_dist.iloc[0] / len(cluster_data) * 100
        print(f"Demografia: {main_gender} dominante ({gender_pct:.0f}%)")
    
    # Sample Likert originali per validazione
    likert_sample = ['q8', 'q13', 'q19', 'q21']
    existing_sample = [var for var in likert_sample if var in cluster_data.columns]
    if existing_sample:
        likert_means = [cluster_data[var].mean() for var in existing_sample]
        avg_likert = np.mean(likert_means)
        print(f"Likert medi: {avg_likert:.1f}")
    
    # Etichetta interpretativa
    if factor_profile[0] > 0.5 and factor_profile[1] > 0.5:  # Alto Atteggiamenti + Comportamenti
        label = "Eco-Attivisti Completi"
    elif factor_profile[0] > 0.5:  # Solo atteggiamenti alti
        label = "Pro-Ambiente Attitudinali"
    elif factor_profile[1] > 0.5:  # Solo comportamenti alti
        label = "Comportamenti Sostenibili"
    elif factor_profile[2] > 0.5:  # Solo consapevolezza alta
        label = "Consapevoli Base"
    elif all(fp < -0.3 for fp in factor_profile):  # Tutti bassi
        label = "Disimpegnati"
    else:
        label = f"Gruppo Moderato {cluster_id}"
    
    cluster_interpretations[cluster_id] = label
    print(f"Etichetta: '{label}'")

# =============================================================================
# STEP 10: CONFRONTI CON ALTRI METODI
# =============================================================================

print(f"\n=== STEP 10: Confronti con altri metodi ===")

# Confronto con MCA
try:
    mca_results = pd.read_csv('modulo6_mca_results_complete.csv')
    if len(mca_results) == len(df_factor_results):
        ari_fa_mca = adjusted_rand_score(factor_cluster_labels, mca_results['mca_cluster'])
        print(f"ARI Factor Analysis vs MCA: {ari_fa_mca:.3f}")
        
        # Crosstab FA vs MCA
        print(f"\nCrosstab FA vs MCA:")
        crosstab_fa_mca = pd.crosstab(factor_cluster_labels, mca_results['mca_cluster'], margins=True)
        print(crosstab_fa_mca)
except:
    print("MCA non disponibile per confronto")

# Confronto con LCA
try:
    lca_results = pd.read_csv('modulo7_lca_sas_results.csv')
    if len(lca_results) == len(df_factor_results):
        ari_fa_lca = adjusted_rand_score(factor_cluster_labels, lca_results['lca_class'])
        print(f"ARI Factor Analysis vs LCA: {ari_fa_lca:.3f}")
except:
    print("LCA non disponibile per confronto")

# Confronto con Hierarchical
try:
    hier_results = pd.read_csv('modulo5_hierarchical_results.csv')
    if len(hier_results) == len(df_factor_results):
        ari_fa_hier = adjusted_rand_score(factor_cluster_labels, hier_results['hier_cluster'])
        print(f"ARI Factor Analysis vs Hierarchical: {ari_fa_hier:.3f}")
except:
    print("Hierarchical non disponibile per confronto")

# =============================================================================
# STEP 11: VISUALIZZAZIONI
# =============================================================================

print(f"\n=== STEP 11: Visualizzazioni ===")

# 1. Heatmap factor loadings
plt.figure(figsize=(12, 8))
sns.heatmap(loadings_df, annot=True, cmap='RdBu_r', center=0, fmt='.2f',
            cbar_kws={'label': 'Factor Loading'})
plt.title('Matrix Factor Loadings (Varimax Rotation)')
plt.tight_layout()
plt.savefig('modulo8_factor_loadings.png', dpi=300, bbox_inches='tight')
plt.show()

# 2. Scatter plot cluster sui primi 2 fattori
plt.figure(figsize=(12, 8))
colors = plt.cm.Set1(np.linspace(0, 1, optimal_k_factors))

for i in range(optimal_k_factors):
    mask = factor_cluster_labels == i
    plt.scatter(factor_scores[mask, 0], factor_scores[mask, 1],
               c=[colors[i]], alpha=0.7, s=50,
               label=f'C{i}: {cluster_interpretations[i]} (n={mask.sum()})')

plt.xlabel(f'Factor 1: {factor_interpretations["Factor_1"]}')
plt.ylabel(f'Factor 2: {factor_interpretations["Factor_2"]}') 
plt.title('Clustering nello Spazio Fattoriale')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('modulo8_factor_clustering.png', dpi=300, bbox_inches='tight')
plt.show()

# 3. Profili cluster sui fattori
cluster_profiles = []
cluster_labels_plot = []

for i in range(optimal_k_factors):
    mask = factor_cluster_labels == i
    profile = np.mean(factor_scores[mask], axis=0)
    cluster_profiles.append(profile)
    cluster_labels_plot.append(f'C{i}: {cluster_interpretations[i]}')

profiles_array = np.array(cluster_profiles)

plt.figure(figsize=(10, 8))
sns.heatmap(profiles_array, annot=True, fmt='.2f', cmap='RdBu_r', center=0,
            xticklabels=[factor_interpretations[f] for f in factor_names],
            yticklabels=cluster_labels_plot,
            cbar_kws={'label': 'Factor Score Medio'})
plt.title('Profili Cluster sui Fattori')
plt.tight_layout()
plt.savefig('modulo8_cluster_profiles.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# STEP 12: EXPORT RISULTATI
# =============================================================================

print(f"\n=== STEP 12: Export risultati ===")

# Dataset finale
df_export = df_factor_results.copy()
df_export['factor_cluster_label'] = df_export['factor_cluster'].map(cluster_interpretations)

# Export principale
df_export.to_csv('modulo8_factor_analysis_results.csv', index=False)

# Export factor loadings
loadings_df.to_csv('modulo8_factor_loadings.csv')

# Export cluster profiles sui fattori
profiles_export = pd.DataFrame(profiles_array, 
                              columns=factor_names,
                              index=cluster_labels_plot)
profiles_export.to_csv('modulo8_cluster_factor_profiles.csv')

# =============================================================================
# STEP 13: RIEPILOGO FINALE TUTTI I METODI
# =============================================================================

print(f"\n=== RIEPILOGO FINALE - TUTTI I METODI ===")

methods_summary = {
    'Metodo': ['Hierarchical Ward', 'MCA Clustering', 'LCA Gaussian', 'Factor Analysis'],
    'K_Clusters': [2, 3, 5, optimal_k_factors],
    'Silhouette': [0.081, 0.427, 0.066, final_silhouette],
    'Approccio': ['Divisione binaria', 'Socioeconomico', 'Genere+Atteggiamenti', 'Fattori attitudinali'],
    'Dataset': ['Originale', 'Categoriali', 'SAS mixed', 'Likert puro']
}

summary_df = pd.DataFrame(methods_summary)
print(summary_df)

# Ranking per qualità
print(f"\nRanking per Silhouette Score:")
sorted_methods = summary_df.sort_values('Silhouette', ascending=False)
for i, (_, row) in enumerate(sorted_methods.iterrows(), 1):
    print(f"{i}. {row['Metodo']}: {row['Silhouette']:.3f} (K={row['K_Clusters']})")

# Triangolazione insights
print(f"\n=== TRIANGOLAZIONE INSIGHTS ===")
print("1. MCA (0.427): Segmentazione socioeconomica dominante")
print("2. Factor Analysis (0.263): Fattori attitudinali puri")
print("3. Hierarchical (0.081): Divisione demografica base")
print("4. LCA (0.066): Pattern genere + educazione")
print("\nCONCLUSIONE: Struttura multidimensionale con componente")
print("socioeconomica forte + fattori attitudinali secondari")

print(f"\n=== MODULO 8 FACTOR ANALYSIS COMPLETATO ===")
print(f"Fattori estratti: {optimal_n_factors}")
print(f"Cluster sui fattori: {optimal_k_factors}")
print(f"Silhouette: {final_silhouette:.3f}")
print("Approccio puramente attitudinale implementato con successo")
