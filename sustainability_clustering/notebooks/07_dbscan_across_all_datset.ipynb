# =============================================================================
# MODULO CROSS-DATASET 3: DBSCAN SU TUTTI I DATASET
# Density-based clustering - trova cluster di forma arbitraria
# =============================================================================

import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.neighbors import NearestNeighbors
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO CROSS-DATASET 3: DBSCAN CLUSTERING ===")

# DBSCAN parameters to test
eps_range = [0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0]
min_samples_range = [3, 5, 10, 15]

dbscan_results = []

print(f"DBSCAN parameters:")
print(f"eps range: {eps_range}")
print(f"min_samples range: {min_samples_range}")

for dataset_name, data in datasets.items():
    print(f"\nTesting {dataset_name}...")
    
    # Standardizzazione
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    
    # Stima eps ottimale usando k-distance graph
    k = 4  # Tipicamente min_samples - 1
    neigh = NearestNeighbors(n_neighbors=k)
    neigh.fit(data_scaled)
    distances, indices = neigh.kneighbors(data_scaled)
    distances = np.sort(distances, axis=0)
    distances = distances[:,1]  # k-th nearest neighbor distance
    
    # Eps suggerito (punto del gomito approssimativo)
    eps_suggested = np.percentile(distances, 90)
    print(f"  Eps suggerito (90th percentile k-dist): {eps_suggested:.3f}")
    
    for eps in eps_range:
        for min_samples in min_samples_range:
            try:
                dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                labels = dbscan.fit_predict(data_scaled)
                
                # Analisi risultati
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                n_core = len(dbscan.core_sample_indices_)
                
                # Solo se abbiamo almeno 2 cluster
                if n_clusters >= 2:
                    # Calcola metriche solo sui punti non-noise
                    mask_no_noise = labels != -1
                    if np.sum(mask_no_noise) > 0 and len(np.unique(labels[mask_no_noise])) > 1:
                        
                        silhouette = silhouette_score(data_scaled[mask_no_noise], 
                                                    labels[mask_no_noise])
                        calinski_harabasz = calinski_harabasz_score(data_scaled[mask_no_noise], 
                                                                   labels[mask_no_noise])
                        
                        dbscan_results.append({
                            'Dataset': dataset_name,
                            'eps': eps,
                            'min_samples': min_samples,
                            'n_clusters': n_clusters,
                            'n_noise': n_noise,
                            'noise_ratio': n_noise / len(labels),
                            'n_core': n_core,
                            'Silhouette': silhouette,
                            'Calinski_Harabasz': calinski_harabasz,
                            'N_Features': data.shape[1]
                        })
                        
                        print(f"    eps={eps:.1f}, min_samples={min_samples}: "
                              f"clusters={n_clusters}, noise={n_noise}({n_noise/len(labels)*100:.1f}%), "
                              f"Sil={silhouette:.3f}")
                else:
                    if n_clusters == 0:
                        print(f"    eps={eps:.1f}, min_samples={min_samples}: Tutti noise")
                    elif n_clusters == 1:
                        print(f"    eps={eps:.1f}, min_samples={min_samples}: 1 solo cluster")
                        
            except Exception as e:
                print(f"    eps={eps:.1f}, min_samples={min_samples}: Errore - {e}")

dbscan_df = pd.DataFrame(dbscan_results)
print(f"\nRisultati DBSCAN validi: {len(dbscan_df)}")

if len(dbscan_df) > 0:
    # Analisi risultati
    print(f"\n=== Analisi DBSCAN Results ===")
    
    # Migliori per dataset
    print("Migliori Silhouette per dataset:")
    for dataset_name in datasets.keys():
        dataset_results = dbscan_df[dbscan_df['Dataset'] == dataset_name]
        if len(dataset_results) > 0:
            best_row = dataset_results.loc[dataset_results['Silhouette'].idxmax()]
            print(f"  {dataset_name}: eps={best_row['eps']:.1f}, min_samples={best_row['min_samples']}, "
                  f"clusters={best_row['n_clusters']}, noise={best_row['noise_ratio']*100:.1f}%, "
                  f"Sil={best_row['Silhouette']:.3f}")
        else:
            print(f"  {dataset_name}: Nessun risultato valido")
    
    # Top 10 complessivo
    print(f"\nTop 10 DBSCAN (tutti dataset):")
    top_dbscan = dbscan_df.nlargest(10, 'Silhouette')
    for _, row in top_dbscan.iterrows():
        print(f"  {row['Dataset']} eps={row['eps']:.1f} min_samples={row['min_samples']}: "
              f"Sil={row['Silhouette']:.3f}, clusters={row['n_clusters']}, "
              f"noise={row['noise_ratio']*100:.1f}%")
    
    # Analisi noise patterns
    print(f"\nAnalisi Noise Ratios:")
    noise_stats = dbscan_df.groupby('Dataset')['noise_ratio'].agg(['mean', 'min', 'max'])
    for dataset_name, stats in noise_stats.iterrows():
        print(f"  {dataset_name}: noise {stats['mean']*100:.1f}% Â± "
              f"{(stats['max']-stats['min'])*50:.1f}% (range {stats['min']*100:.1f}-{stats['max']*100:.1f}%)")

else:
    print("DBSCAN non ha prodotto clustering validi con i parametri testati")
    print("Possibili cause: dataset troppo piccolo o parametri non appropriati")

print("=== Procedo con Spectral Clustering ===")
