# =============================================================================
# MODULO TEST SOM - SELF-ORGANIZING MAPS
# =============================================================================


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO SOM - SELF-ORGANIZING MAPS===")

# Per ora usiamo una implementazione SOM semplificata senza minisom
# che potrebbe non essere disponibile

def simple_som_implementation(data, som_width=6, som_height=6, learning_rate=0.5, iterations=500):
    """
    Implementazione SOM semplificata senza dipendenze esterne
    """
    n_samples, n_features = data.shape
    
    # Inizializza pesi random
    np.random.seed(42)
    weights = np.random.random((som_width, som_height, n_features))
    
    print(f"Inizializzando SOM {som_width}x{som_height} con {iterations} iterazioni...")
    
    # Training
    for iteration in range(iterations):
        # Decadimento learning rate e neighborhood radius
        current_lr = learning_rate * (1 - iteration/iterations)
        current_radius = max(1, som_width * (1 - iteration/iterations))
        
        # Per ogni sample
        for sample in data:
            # Trova BMU (Best Matching Unit)
            distances = np.sum((weights - sample)**2, axis=2)
            bmu_idx = np.unravel_index(np.argmin(distances), distances.shape)
            
            # Aggiorna pesi nel neighborhood
            for i in range(som_width):
                for j in range(som_height):
                    # Distanza dal BMU
                    dist_to_bmu = np.sqrt((i - bmu_idx[0])**2 + (j - bmu_idx[1])**2)
                    
                    if dist_to_bmu <= current_radius:
                        # Influence function (Gaussiana)
                        influence = np.exp(-(dist_to_bmu**2) / (2 * (current_radius**2)))
                        
                        # Update rule
                        weights[i, j] += current_lr * influence * (sample - weights[i, j])
        
        if (iteration + 1) % 100 == 0:
            print(f"  Iterazione {iteration + 1}/{iterations} completata")
    
    return weights

def find_bmu_for_samples(data, som_weights):
    """Trova Best Matching Unit per ogni sample"""
    som_width, som_height = som_weights.shape[0], som_weights.shape[1]
    bmu_coords = []
    
    for sample in data:
        distances = np.sum((som_weights - sample)**2, axis=2)
        bmu_idx = np.unravel_index(np.argmin(distances), distances.shape)
        bmu_coords.append(bmu_idx)
    
    return np.array(bmu_coords)

def create_distance_map(som_weights):
    """Crea U-matrix (distanze tra neuroni adiacenti)"""
    som_width, som_height = som_weights.shape[0], som_weights.shape[1]
    u_matrix = np.zeros((som_width, som_height))
    
    for i in range(som_width):
        for j in range(som_height):
            neighbors = []
            # Controllo neuroni adiacenti
            for di in [-1, 0, 1]:
                for dj in [-1, 0, 1]:
                    ni, nj = i + di, j + dj
                    if 0 <= ni < som_width and 0 <= nj < som_height and (di != 0 or dj != 0):
                        distance = np.linalg.norm(som_weights[i, j] - som_weights[ni, nj])
                        neighbors.append(distance)
            
            u_matrix[i, j] = np.mean(neighbors) if neighbors else 0
    
    return u_matrix

# Preparazione dataset (stesso del modulo precedente)
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

def prepare_som_data():
    df_encoded = df_original.copy()
    
    df_encoded['eta_norm'] = (df_encoded['q1'] - 18) / (70 - 18)
    df_encoded['genere_donna'] = (df_encoded['q2'] == 'Donna').astype(int)
    df_encoded['titolo_magistrale'] = (df_encoded['q3'] == 'Laurea Magistrale').astype(int)
    df_encoded['occup_studente'] = (df_encoded['q4'] == 'Studente/ Studentessa').astype(int)
    df_encoded['geo_centro'] = (df_encoded['q5'] == 'Centro').astype(int)
    df_encoded['reddito_alto'] = df_encoded['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
    
    # Likert chiave
    likert_vars = ['q8', 'q19', 'q21']
    for var in likert_vars:
        if var in df_encoded.columns:
            df_encoded[f'{var}_norm'] = (df_encoded[var] - 1) / 6

    som_vars = ['eta_norm', 'genere_donna', 'titolo_magistrale', 'occup_studente', 
                'geo_centro', 'reddito_alto'] + [f'{var}_norm' for var in likert_vars if var in df_original.columns]
    
    return df_encoded, som_vars

df_processed, som_vars = prepare_som_data()
X_som = df_processed[som_vars].fillna(df_processed[som_vars].mean()).values

print(f"Dataset SOM: {X_som.shape}")
print(f"Variabili: {som_vars}")

# Standardizzazione
scaler = StandardScaler()
X_som_scaled = scaler.fit_transform(X_som)

# Training SOM
som_weights = simple_som_implementation(X_som_scaled, som_width=6, som_height=6, 
                                       learning_rate=0.3, iterations=300)

print("Training SOM completato")

# Trova BMU per ogni osservazione
bmu_coordinates = find_bmu_for_samples(X_som_scaled, som_weights)

# Converti coordinate in cluster labels
som_labels = []
for coord in bmu_coordinates:
    cluster_id = coord[0] * 6 + coord[1]  # 6x6 grid
    som_labels.append(cluster_id)

som_labels = np.array(som_labels)
n_som_clusters = len(np.unique(som_labels))

print(f"SOM clusters naturali: {n_som_clusters}")
print(f"Distribuzione (primi 10): {np.bincount(som_labels)[:10]}")

# Crea meta-clusters raggruppando neuroni vicini
def create_som_metaclusters_simple(bmu_coords, target_k=3):
    """Crea meta-cluster usando K-means sulle coordinate BMU"""
    # Usa le coordinate dei BMU per clustering
    unique_coords = np.unique(bmu_coords, axis=0)
    
    if len(unique_coords) >= target_k:
        kmeans_meta = KMeans(n_clusters=target_k, random_state=42)
        meta_labels_unique = kmeans_meta.fit_predict(unique_coords)
        
        # Mappa ogni osservazione al meta-cluster
        coord_to_meta = {}
        for i, coord in enumerate(unique_coords):
            coord_to_meta[tuple(coord)] = meta_labels_unique[i]
        
        meta_labels = [coord_to_meta[tuple(coord)] for coord in bmu_coords]
        return np.array(meta_labels)
    else:
        # Fallback: usa le coordinate direttamente
        return bmu_coords[:, 0] % target_k

som_meta_labels = create_som_metaclusters_simple(bmu_coordinates, target_k=3)
print(f"Meta-cluster SOM: {len(np.unique(som_meta_labels))}")
print(f"Distribuzione meta-cluster: {np.bincount(som_meta_labels)}")

# Calcola metriche
if len(np.unique(som_meta_labels)) > 1:
    som_silhouette = silhouette_score(X_som_scaled, som_meta_labels)
    print(f"Silhouette SOM: {som_silhouette:.3f}")
else:
    som_silhouette = 0.0

# Confronto con K-means
kmeans_ref = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans_ref.fit_predict(X_som_scaled)

if len(np.unique(som_meta_labels)) > 1:
    ari_som_kmeans = adjusted_rand_score(som_meta_labels, kmeans_labels)
    print(f"ARI SOM vs K-means: {ari_som_kmeans:.3f}")
else:
    ari_som_kmeans = 0.0

# Visualizzazioni corrette
plt.figure(figsize=(15, 5))

# U-Matrix
plt.subplot(1, 3, 1)
u_matrix = create_distance_map(som_weights)
plt.imshow(u_matrix, cmap='bone_r', origin='lower')
plt.colorbar(label='Distanza media')
plt.title('SOM U-Matrix\n(Distanze tra neuroni)')
plt.xlabel('X')
plt.ylabel('Y')

# Density map
plt.subplot(1, 3, 2)
density_map = np.zeros((6, 6))
for coord in bmu_coordinates:
    density_map[coord[0], coord[1]] += 1

plt.imshow(density_map, cmap='viridis', origin='lower')
plt.colorbar(label='Frequenza')
plt.title('SOM Density Map\n(Attivazioni neuroni)')
plt.xlabel('X')
plt.ylabel('Y')

# Meta-cluster visualization
plt.subplot(1, 3, 3)
# Scatter plot delle coordinate BMU colorate per meta-cluster
colors = ['red', 'blue', 'green', 'orange', 'purple']
for meta_id in range(len(np.unique(som_meta_labels))):
    mask = som_meta_labels == meta_id
    coords_subset = bmu_coordinates[mask]
    plt.scatter(coords_subset[:, 1], coords_subset[:, 0], 
               c=colors[meta_id], label=f'Meta-cluster {meta_id}', 
               alpha=0.6, s=50)

plt.xlabel('X')
plt.ylabel('Y')
plt.title('SOM Meta-Clusters')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Profiling meta-cluster
print(f"\n=== PROFILING META-CLUSTER SOM ===")

for meta_id in range(len(np.unique(som_meta_labels))):
    mask = som_meta_labels == meta_id
    n_obs = np.sum(mask)
    
    if n_obs > 0:
        print(f"\nMETA-CLUSTER SOM {meta_id} ({n_obs} obs, {n_obs/len(som_meta_labels)*100:.1f}%):")
        
        cluster_profile = np.mean(X_som[mask], axis=0)
        
        for i, var_name in enumerate(som_vars):
            var_value = cluster_profile[i]
            if 'norm' in var_name or var_name == 'eta_norm':
                # Variabile normalizzata
                if var_name == 'eta_norm':
                    actual_value = var_value * (70-18) + 18
                    print(f"  {var_name}: {var_value:.3f} (≈{actual_value:.1f} anni)")
                else:
                    print(f"  {var_name}: {var_value:.3f} (scala 0-1)")
            else:
                # Variabile binaria
                pct = var_value * 100
                print(f"  {var_name}: {var_value:.3f} ({pct:.0f}%)")

# Valutazione finale
print(f"\n=== VALUTAZIONE SOM ===")

if som_silhouette > 0.3:
    som_quality = "Buona"
elif som_silhouette > 0.2:
    som_quality = "Accettabile"  
else:
    som_quality = "Limitata"

print(f"Qualità clustering: {som_quality} (Silhouette: {som_silhouette:.3f})")

if ari_som_kmeans > 0.6:
    novelty = "Simile a K-means"
elif ari_som_kmeans > 0.3:
    novelty = "Parzialmente diverso"
else:
    novelty = "Completamente diverso"

print(f"Novità vs K-means: {novelty} (ARI: {ari_som_kmeans:.3f})")

print(f"\n=== INSIGHTS SOM ===")
print("1. Topologia preservata: pattern spaziali visibili nella mappa")
print("2. U-matrix rivela regioni omogenee vs confini di transizione")  
print("3. Density map identifica profili più/meno comuni")
print("4. Approccio non-supervisionato con interpretazione geometrica")

print("\n=== MODULO SOM COMPLETATO ===")
