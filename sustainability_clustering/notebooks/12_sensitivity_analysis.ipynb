# =============================================================================
# MODULO ANALISI DI SENSIBILITÀ
# Test robustezza risultati a variazioni parametri preprocessing e metodologici
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.impute import SimpleImputer, KNNImputer
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO ANALISI DI SENSIBILITÀ ===")

# =============================================================================
# STEP 1: PREPARAZIONE DATASET BASE
# =============================================================================

print("\n=== STEP 1: Caricamento dataset per sensitivity analysis ===")

# Carica dataset originale
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

# Variabili clustering base (dalle analisi precedenti)
base_vars = ['q1', 'q2', 'q3', 'q4', 'q5', 'q6']  # Demografia
likert_vars = ['q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 
               'q16', 'q18', 'q19', 'q20', 'q21', 'q23', 'q24', 'q29']
behavior_vars = ['q15', 'q17', 'q27']

# Filtra variabili esistenti
existing_likert = [var for var in likert_vars if var in df_original.columns]
existing_behavior = [var for var in behavior_vars if var in df_original.columns]

print(f"Dataset base: {df_original.shape}")
print(f"Variabili Likert: {len(existing_likert)}")
print(f"Variabili comportamentali: {len(existing_behavior)}")

# =============================================================================
# STEP 2: DEFINIZIONE PARAMETRI DI SENSIBILITÀ
# =============================================================================

print("\n=== STEP 2: Definizione variazioni parametriche ===")

# Parametri da testare
sensitivity_params = {
    'scalers': {
        'StandardScaler': StandardScaler(),
        'MinMaxScaler': MinMaxScaler(),
        'RobustScaler': RobustScaler(),
        'No_Scaling': None
    },
    
    'imputers': {
        'mean_impute': SimpleImputer(strategy='mean'),
        'median_impute': SimpleImputer(strategy='median'),
        'knn_impute': KNNImputer(n_neighbors=5),
        'constant_impute': SimpleImputer(strategy='constant', fill_value=4)  # Midpoint Likert
    },
    
    'variable_sets': {
        'all_vars': base_vars + existing_likert + existing_behavior,
        'demographic_only': base_vars,
        'likert_only': existing_likert,
        'demo_likert': base_vars + existing_likert,
        'likert_behavior': existing_likert + existing_behavior
    },
    
    'outlier_handling': {
        'keep_all': None,
        'remove_z3': 3.0,  # Remove |z-score| > 3
        'remove_z25': 2.5,  # Remove |z-score| > 2.5
        'iqr_filter': 'iqr'  # Remove IQR outliers
    }
}

print("Parametri da testare:")
for param_type, options in sensitivity_params.items():
    print(f"  {param_type}: {len(options)} opzioni")

# =============================================================================
# STEP 3: PREPROCESSING VARIATIONS
# =============================================================================

print("\n=== STEP 3: Generazione dataset con preprocessing diversi ===")

def encode_demographics(df):
    """Encoding standard delle variabili demografiche"""
    df_encoded = df.copy()
    
    # Età normalizzata
    if 'q1' in df_encoded.columns:
        df_encoded['eta_norm'] = (df_encoded['q1'] - 18) / (70 - 18)
    
    # Genere
    if 'q2' in df_encoded.columns:
        df_encoded['genere_donna'] = (df_encoded['q2'] == 'Donna').astype(int)
    
    # Titolo di studio
    if 'q3' in df_encoded.columns:
        df_encoded['titolo_magistrale'] = (df_encoded['q3'] == 'Laurea Magistrale').astype(int)
        df_encoded['titolo_postlaurea'] = (df_encoded['q3'] == 'Formazione Post Laurea (Master/Dottorato)').astype(int)
    
    # Occupazione
    if 'q4' in df_encoded.columns:
        df_encoded['occup_studente'] = (df_encoded['q4'] == 'Studente/ Studentessa').astype(int)
        df_encoded['occup_privato'] = (df_encoded['q4'] == 'Lavoro dipendente privato').astype(int)
    
    # Geografia
    if 'q5' in df_encoded.columns:
        df_encoded['geo_nord'] = df_encoded['q5'].isin(['Nord Est', 'Nord Ovest']).astype(int)
        df_encoded['geo_centro'] = (df_encoded['q5'] == 'Centro').astype(int)
    
    # Reddito
    if 'q6' in df_encoded.columns:
        df_encoded['reddito_alto'] = df_encoded['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
        df_encoded['reddito_basso'] = (df_encoded['q6'] == 'Meno di 15000').astype(int)
    
    return df_encoded

def remove_outliers(data, method, threshold=3.0):
    """Rimuove outliers con diversi metodi"""
    if method is None:
        return data, np.ones(len(data), dtype=bool)
    
    if method == 'iqr':
        Q1 = np.percentile(data, 25, axis=0)
        Q3 = np.percentile(data, 75, axis=0)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        mask = np.all((data >= lower) & (data <= upper), axis=1)
    else:  # z-score based
        z_scores = np.abs((data - np.mean(data, axis=0)) / np.std(data, axis=0))
        mask = np.all(z_scores <= threshold, axis=1)
    
    return data[mask], mask

# Genera dataset preprocessati
preprocessed_datasets = {}
preprocessing_log = []

# Encoding demografico base
df_encoded = encode_demographics(df_original)

demo_encoded_vars = ['eta_norm', 'genere_donna', 'titolo_magistrale', 'titolo_postlaurea', 
                     'occup_studente', 'occup_privato', 'geo_nord', 'geo_centro', 
                     'reddito_alto', 'reddito_basso']

# Aggiorna variable sets con encoding
sensitivity_params['variable_sets']['demographic_encoded'] = demo_encoded_vars
sensitivity_params['variable_sets']['full_encoded'] = demo_encoded_vars + existing_likert + existing_behavior

dataset_id = 0

for scaler_name, scaler in sensitivity_params['scalers'].items():
    for imputer_name, imputer in sensitivity_params['imputers'].items():
        for varset_name, variables in sensitivity_params['variable_sets'].items():
            for outlier_name, outlier_method in sensitivity_params['outlier_handling'].items():
                
                try:
                    # Filtra variabili esistenti
                    available_vars = [var for var in variables if var in df_encoded.columns]
                    
                    if len(available_vars) < 3:
                        continue  # Skip se troppe poche variabili
                    
                    # Estrai dati
                    data = df_encoded[available_vars].copy()
                    
                    # Impute missing values
                    if imputer is not None:
                        data_imputed = pd.DataFrame(
                            imputer.fit_transform(data), 
                            columns=data.columns,
                            index=data.index
                        )
                    else:
                        data_imputed = data.fillna(data.mean())
                    
                    # Remove outliers
                    data_clean, outlier_mask = remove_outliers(
                        data_imputed.values, 
                        outlier_method,
                        threshold=3.0 if outlier_name == 'remove_z3' else 2.5
                    )
                    
                    if data_clean.shape[0] < 50:  # Minimo campione
                        continue
                    
                    # Scaling
                    if scaler is not None:
                        data_final = scaler.fit_transform(data_clean)
                    else:
                        data_final = data_clean
                    
                    # Salva dataset
                    config_name = f"{scaler_name}_{imputer_name}_{varset_name}_{outlier_name}"
                    preprocessed_datasets[config_name] = {
                        'data': data_final,
                        'n_samples': data_final.shape[0],
                        'n_features': data_final.shape[1],
                        'config': {
                            'scaler': scaler_name,
                            'imputer': imputer_name,
                            'variables': varset_name,
                            'outlier': outlier_name
                        }
                    }
                    
                    preprocessing_log.append({
                        'config': config_name,
                        'n_samples': data_final.shape[0],
                        'n_features': data_final.shape[1],
                        'outliers_removed': len(df_encoded) - data_final.shape[0]
                    })
                    
                    dataset_id += 1
                    
                except Exception as e:
                    continue

print(f"Dataset preprocessati generati: {len(preprocessed_datasets)}")
print(f"Configurazioni valide: {dataset_id}")

# Mostra sample delle configurazioni
print("\nSample configurazioni:")
for i, (config_name, dataset_info) in enumerate(list(preprocessed_datasets.items())[:5]):
    print(f"{i+1}. {config_name}: {dataset_info['n_samples']} samples, {dataset_info['n_features']} features")

# =============================================================================
# STEP 4: TEST CLUSTERING SENSITIVITY
# =============================================================================

print("\n=== STEP 4: Test sensibilità clustering ===")

# Metodi clustering da testare (focus sui migliori dalle analisi precedenti)
clustering_methods = {
    'KMeans_2': {'method': 'kmeans', 'params': {'n_clusters': 2, 'random_state': 42}},
    'KMeans_3': {'method': 'kmeans', 'params': {'n_clusters': 3, 'random_state': 42}},
    'Hierarchical_2': {'method': 'hierarchical', 'params': {'n_clusters': 2, 'linkage': 'ward'}},
    'Hierarchical_3': {'method': 'hierarchical', 'params': {'n_clusters': 3, 'linkage': 'average'}}
}

sensitivity_results = []

print(f"Testing {len(clustering_methods)} metodi su {len(preprocessed_datasets)} configurazioni...")

for config_name, dataset_info in preprocessed_datasets.items():
    data = dataset_info['data']
    config = dataset_info['config']
    
    if len(preprocessed_datasets) > 20 and config_name not in list(preprocessed_datasets.keys())[::5]:
        continue  # Sample per velocità se troppe configurazioni
    
    for method_name, method_info in clustering_methods.items():
        try:
            # Clustering
            if method_info['method'] == 'kmeans':
                clusterer = KMeans(**method_info['params'])
            else:  # hierarchical
                clusterer = AgglomerativeClustering(**method_info['params'])
            
            labels = clusterer.fit_predict(data)
            
            # Metriche
            n_clusters_found = len(np.unique(labels))
            if n_clusters_found > 1:
                silhouette = silhouette_score(data, labels)
            else:
                silhouette = 0.0
            
            sensitivity_results.append({
                'config_name': config_name,
                'method': method_name,
                'scaler': config['scaler'],
                'imputer': config['imputer'],
                'variables': config['variables'],
                'outlier_handling': config['outlier'],
                'n_samples': dataset_info['n_samples'],
                'n_features': dataset_info['n_features'],
                'n_clusters_found': n_clusters_found,
                'silhouette': silhouette,
                'valid': n_clusters_found == method_info['params']['n_clusters']
            })
            
        except Exception as e:
            continue

sensitivity_df = pd.DataFrame(sensitivity_results)
print(f"Risultati sensitivity: {len(sensitivity_df)} test completati")

# Filtra solo risultati validi
valid_results = sensitivity_df[sensitivity_df['valid'] == True]
print(f"Risultati validi: {len(valid_results)}/{len(sensitivity_df)} ({len(valid_results)/len(sensitivity_df)*100:.1f}%)")

# =============================================================================
# STEP 5: ANALISI RISULTATI SENSIBILITÀ
# =============================================================================

print("\n=== STEP 5: Analisi risultati sensibilità ===")

if len(valid_results) > 0:
    # 1. Sensibilità per parametro
    print("1. SENSIBILITÀ PER PARAMETRO:")
    
    for param in ['scaler', 'imputer', 'variables', 'outlier_handling']:
        param_stats = valid_results.groupby(param)['silhouette'].agg(['mean', 'std', 'count'])
        print(f"\n{param.upper()}:")
        for param_val, stats in param_stats.iterrows():
            print(f"  {param_val}: μ={stats['mean']:.3f} ± {stats['std']:.3f} (n={stats['count']})")
    
    # 2. Migliori e peggiori configurazioni
    print(f"\n2. TOP 5 CONFIGURAZIONI:")
    top_configs = valid_results.nlargest(5, 'silhouette')
    for _, row in top_configs.iterrows():
        print(f"  {row['method']}: {row['silhouette']:.3f} - {row['scaler']}/{row['imputer']}/{row['variables']}")
    
    print(f"\nWORST 5 CONFIGURAZIONI:")
    worst_configs = valid_results.nsmallest(5, 'silhouette') 
    for _, row in worst_configs.iterrows():
        print(f"  {row['method']}: {row['silhouette']:.3f} - {row['scaler']}/{row['imputer']}/{row['variables']}")
    
    # 3. Robustezza per metodo
    print(f"\n3. ROBUSTEZZA PER METODO CLUSTERING:")
    method_stats = valid_results.groupby('method')['silhouette'].agg(['mean', 'std', 'min', 'max', 'count'])
    for method, stats in method_stats.iterrows():
        cv = stats['std'] / stats['mean'] if stats['mean'] > 0 else np.inf
        robustness = "Alta" if cv < 0.2 else "Media" if cv < 0.5 else "Bassa"
        print(f"  {method}: μ={stats['mean']:.3f} ± {stats['std']:.3f}, range=[{stats['min']:.3f}-{stats['max']:.3f}], CV={cv:.3f} ({robustness})")
    
    # 4. Varianza explained per parametro
    print(f"\n4. VARIANZA SPIEGATA DA OGNI PARAMETRO:")
    
    total_variance = valid_results['silhouette'].var()
    
    for param in ['scaler', 'imputer', 'variables', 'outlier_handling', 'method']:
        # ANOVA-like variance decomposition
        group_means = valid_results.groupby(param)['silhouette'].mean()
        group_counts = valid_results.groupby(param).size()
        
        between_variance = np.sum(group_counts * (group_means - valid_results['silhouette'].mean())**2) / (len(group_means) - 1)
        variance_explained = between_variance / total_variance if total_variance > 0 else 0
        
        print(f"  {param}: {variance_explained*100:.1f}% della varianza")

else:
    print("ERRORE: Nessun risultato valido trovato!")
    print("Possibili cause:")
    print("- Dataset troppo piccolo dopo preprocessing")
    print("- Parametri clustering non appropriati")
    print("- Errori nel preprocessing")

print(f"\n=== RIEPILOGO SENSITIVITY ANALYSIS ===")
if len(valid_results) > 0:
    overall_mean = valid_results['silhouette'].mean()
    overall_std = valid_results['silhouette'].std()
    overall_cv = overall_std / overall_mean if overall_mean > 0 else 0
    
    print(f"Performance media: {overall_mean:.3f} ± {overall_std:.3f}")
    print(f"Coefficiente variazione: {overall_cv:.3f}")
    
    if overall_cv < 0.2:
        print("✓ RISULTATI ROBUSTI: Bassa sensibilità ai parametri")
    elif overall_cv < 0.5:
        print("⚠ RISULTATI MODERATI: Media sensibilità ai parametri")
    else:
        print("✗ RISULTATI INSTABILI: Alta sensibilità ai parametri")
    
    # Identificazione parametri più critici
    param_importance = {}
    for param in ['scaler', 'imputer', 'variables', 'outlier_handling']:
        param_variance = valid_results.groupby(param)['silhouette'].var().mean()
        param_importance[param] = param_variance
    
    most_critical = max(param_importance, key=param_importance.get)
    print(f"Parametro più critico: {most_critical}")
    
else:
    print("✗ ANALISI FALLITA: Rivedere configurazione parametri")

print("\n=== MODULO ANALISI DI SENSIBILITÀ COMPLETATO ===")
