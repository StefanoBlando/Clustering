# =============================================================================
# MODULO 1: PREPROCESSING BASE E CLUSTERING FONDAMENTALE
# Equivalente Python dei moduli SAS base - preprocessing e clustering iniziale
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from scipy import stats
from scipy.stats import f_oneway
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO 1: PREPROCESSING BASE E CLUSTERING FONDAMENTALE ===")

# =============================================================================
# STEP 1: CARICAMENTO E ESPLORAZIONE DATI
# =============================================================================

print("\n=== STEP 1: Caricamento dataset originale ===")

# Carica il dataset
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

print(f"Dataset caricato: {df_original.shape}")
print(f"Variabili disponibili: {df_original.columns.tolist()}")

# Analisi missing values
print(f"\nAnalisi missing values:")
missing_analysis = df_original.isnull().sum()
missing_pct = (missing_analysis / len(df_original) * 100).round(2)
missing_summary = pd.DataFrame({
    'Missing_Count': missing_analysis,
    'Missing_Pct': missing_pct
}).sort_values('Missing_Count', ascending=False)

print(missing_summary.head(10))

# Statistiche descrittive base
print(f"\nStatistiche descrittive variabili numeriche:")
numeric_vars = df_original.select_dtypes(include=[np.number]).columns
if len(numeric_vars) > 0:
    print(df_original[numeric_vars].describe())

# =============================================================================
# STEP 2: PREPROCESSING DEMOGRAFICO
# =============================================================================

print("\n=== STEP 2: Preprocessing variabili demografiche ===")

# Crea dataset preprocessato
df_processed = df_original.copy()

# Crea colonna ID per compatibilità con moduli successivi
df_processed['id'] = range(1, len(df_processed) + 1)
print(f"Colonna ID creata: {len(df_processed)} osservazioni")

# Età normalizzata (q1)
if 'q1' in df_processed.columns:
    df_processed['eta'] = (df_processed['q1'] - 18) / (70 - 18)
    print(f"Età normalizzata: range {df_processed['eta'].min():.2f} - {df_processed['eta'].max():.2f}")

# Genere (q2)
if 'q2' in df_processed.columns:
    df_processed['genere_donna'] = (df_processed['q2'] == 'Donna').astype(int)
    genere_dist = df_processed['genere_donna'].value_counts()
    print(f"Distribuzione genere: Uomini {genere_dist[0]}, Donne {genere_dist[1]}")

# Titolo di studio (q3)
if 'q3' in df_processed.columns:
    df_processed['titolo_magistrale'] = (df_processed['q3'] == 'Laurea Magistrale').astype(int)
    df_processed['titolo_postlaurea'] = (df_processed['q3'] == 'Formazione Post Laurea (Master/Dottorato)').astype(int)
    print(f"Titolo magistrale: {df_processed['titolo_magistrale'].sum()} persone")
    print(f"Titolo post-laurea: {df_processed['titolo_postlaurea'].sum()} persone")

# Occupazione (q4)
if 'q4' in df_processed.columns:
    df_processed['occup_studente'] = (df_processed['q4'] == 'Studente/ Studentessa').astype(int)
    df_processed['occup_privato'] = (df_processed['q4'] == 'Lavoro dipendente privato').astype(int)
    df_processed['occup_pubblico'] = (df_processed['q4'] == 'Lavoro dipendente pubblico').astype(int)
    print(f"Studenti: {df_processed['occup_studente'].sum()}")
    print(f"Dipendenti privati: {df_processed['occup_privato'].sum()}")

# Geografia (q5)
if 'q5' in df_processed.columns:
    df_processed['geo_nord'] = df_processed['q5'].isin(['Nord Est', 'Nord Ovest']).astype(int)
    df_processed['geo_centro'] = (df_processed['q5'] == 'Centro').astype(int)
    df_processed['geo_sud'] = (df_processed['q5'] == 'Sud').astype(int)
    df_processed['geo_isole'] = (df_processed['q5'] == 'Isole').astype(int)
    print(f"Distribuzione geografica:")
    print(f"  Nord: {df_processed['geo_nord'].sum()}")
    print(f"  Centro: {df_processed['geo_centro'].sum()}")
    print(f"  Sud: {df_processed['geo_sud'].sum()}")
    print(f"  Isole: {df_processed['geo_isole'].sum()}")

# Reddito (q6)
if 'q6' in df_processed.columns:
    df_processed['reddito_basso'] = (df_processed['q6'] == 'Meno di 15000').astype(int)
    df_processed['reddito_medio'] = df_processed['q6'].isin(['15001 - 30000', '30001 - 50000']).astype(int)
    df_processed['reddito_alto'] = (df_processed['q6'] == 'Più di 50000').astype(int)
    df_processed['reddito_medio_alto'] = df_processed['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
    print(f"Distribuzione reddito:")
    print(f"  Basso (<15K): {df_processed['reddito_basso'].sum()}")
    print(f"  Medio (15-50K): {df_processed['reddito_medio'].sum()}")
    print(f"  Alto (>50K): {df_processed['reddito_alto'].sum()}")

# =============================================================================
# STEP 3: PREPROCESSING VARIABILI LIKERT
# =============================================================================

print("\n=== STEP 3: Preprocessing variabili Likert ===")

# Identifica variabili Likert 1-5 (comportamenti)
likert_5_vars = ['q15', 'q17', 'q27']
likert_5_mapping = {'Mai': 1, 'Raramente': 2, 'Qualche volta': 3, 'Spesso': 4, 'Sempre': 5}

for var in likert_5_vars:
    if var in df_processed.columns:
        df_processed[f'lik5_{var}'] = df_processed[var].map(likert_5_mapping)
        # Imputa missing con mediana
        if df_processed[f'lik5_{var}'].isnull().any():
            median_val = df_processed[f'lik5_{var}'].median()
            df_processed[f'lik5_{var}'] = df_processed[f'lik5_{var}'].fillna(median_val)
        print(f"{var} -> lik5_{var}: media {df_processed[f'lik5_{var}'].mean():.2f}")

# Identifica variabili Likert 1-7 (atteggiamenti)
likert_7_vars = ['q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 
                 'q16', 'q18', 'q19', 'q20', 'q21', 'q23', 'q24', 'q29']

existing_likert_7 = [var for var in likert_7_vars if var in df_processed.columns]
print(f"Variabili Likert 1-7 trovate: {len(existing_likert_7)}")

# Imputa missing con mediana e prepara per size effect
likert_7_processed = []
for var in existing_likert_7:
    if var in df_processed.columns:
        # Imputa missing con mediana
        median_val = df_processed[var].median()
        df_processed[var] = df_processed[var].fillna(median_val)
        
        # Crea versione processata
        df_processed[f'lik7_{var}'] = df_processed[var]
        likert_7_processed.append(f'lik7_{var}')

# Calcola individual response style (size effect) se abbiamo abbastanza variabili
if len(likert_7_processed) >= 3:
    df_processed['individual_mean'] = df_processed[likert_7_processed].mean(axis=1)
    overall_mean = df_processed[likert_7_processed].values.mean()
    
    # Size effect corrected - crea nuove colonne
    for var in likert_7_processed:
        df_processed[f'{var}c'] = (df_processed[var] - df_processed['individual_mean'] + overall_mean)
    
    print(f"Size effect correction applicata a {len(likert_7_processed)} variabili")
    print(f"Nuove variabili create con suffisso 'c' (corrected)")

# =============================================================================
# STEP 4: PREPROCESSING VARIABILI COMPORTAMENTALI
# =============================================================================

print("\n=== STEP 4: Preprocessing variabili comportamentali ===")

# Trasporto sostenibile (q22)
if 'q22' in df_processed.columns:
    trasporto_sostenibile = ['Bicicletta o a piedi', 'Mezzo privato (ibrido/elettrico)']
    df_processed['trasporto_sostenibile'] = df_processed['q22'].isin(trasporto_sostenibile).astype(int)
    print(f"Trasporto sostenibile: {df_processed['trasporto_sostenibile'].sum()} persone ({df_processed['trasporto_sostenibile'].mean()*100:.1f}%)")

# Ostacoli principali (q25)
if 'q25' in df_processed.columns:
    df_processed['ostacolo_costi'] = (df_processed['q25'] == 'I costi più elevati').astype(int)
    df_processed['ostacolo_informazioni'] = (df_processed['q25'] == 'La mancanza di informazioni chiare').astype(int)
    df_processed['ostacolo_disponibilita'] = (df_processed['q25'] == 'La scarsa disponibilità di alternative sostenibili').astype(int)
    print(f"Ostacolo costi: {df_processed['ostacolo_costi'].sum()} persone")

# Responsabilità percepita (q26)
if 'q26' in df_processed.columns:
    df_processed['resp_governi'] = (df_processed['q26'] == 'I governi e le istituzioni internazionali').astype(int)
    df_processed['resp_aziende'] = (df_processed['q26'] == 'Le aziende e le imprese').astype(int)
    df_processed['resp_individui'] = (df_processed['q26'] == 'I singoli cittadini').astype(int)
    print(f"Responsabilità governi: {df_processed['resp_governi'].sum()} persone")

# Partecipazione attiva (q28)
if 'q28' in df_processed.columns:
    partecipazione_attiva = ['Sì, più di una volta', 'Sì, spesso']
    df_processed['partec_attiva'] = df_processed['q28'].isin(partecipazione_attiva).astype(int)
    print(f"Partecipazione attiva: {df_processed['partec_attiva'].sum()} persone")

# =============================================================================
# STEP 5: CREAZIONE DATASET CLUSTERING
# =============================================================================

print("\n=== STEP 5: Creazione dataset per clustering ===")

# Variabili clustering finali
clustering_vars = [
    # Demografia base
    'eta', 'genere_donna', 'titolo_magistrale', 'occup_studente', 
    'geo_isole', 'reddito_medio_alto',
    
    # Comportamenti Likert 1-5
    'lik5_q15', 'lik5_q17', 'lik5_q27',
    
    # Comportamenti specifici
    'trasporto_sostenibile', 'ostacolo_costi', 'resp_governi', 'partec_attiva'
]

# Aggiungi Likert 1-7 corretti (se disponibili)
likert_corrected_available = [f'lik7_{var}c' for var in existing_likert_7 
                             if f'lik7_{var}c' in df_processed.columns]
clustering_vars.extend(likert_corrected_available[:6])  # Massimo 6 per non sovraccaricare

# Filtra variabili esistenti
clustering_vars_final = [var for var in clustering_vars if var in df_processed.columns]

print(f"Variabili clustering selezionate: {len(clustering_vars_final)}")
for var in clustering_vars_final:
    if var in df_processed.columns:
        print(f"  - {var}")

# Crea dataset clustering
X_clustering = df_processed[clustering_vars_final].copy()

# Gestisci missing values
initial_missing = X_clustering.isnull().sum().sum()
X_clustering = X_clustering.fillna(X_clustering.mean())
print(f"Missing values imputati: {initial_missing}")

print(f"Dataset clustering finale: {X_clustering.shape}")

# =============================================================================
# STEP 6: STANDARDIZZAZIONE E CLUSTERING BASE
# =============================================================================

print("\n=== STEP 6: Standardizzazione e clustering base ===")

# Standardizzazione
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_clustering)

print("Dataset standardizzato")

# K-means per diversi K
k_range = range(2, 7)
kmeans_results = []

print("Testing K-means per diversi K:")
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    
    silhouette = silhouette_score(X_scaled, labels)
    calinski_harabasz = calinski_harabasz_score(X_scaled, labels)
    inertia = kmeans.inertia_
    
    kmeans_results.append({
        'k': k,
        'silhouette': silhouette,
        'calinski_harabasz': calinski_harabasz,
        'inertia': inertia,
        'labels': labels
    })
    
    print(f"  K={k}: Silhouette={silhouette:.3f}, CH={calinski_harabasz:.1f}")

# Trova K ottimale
best_k = max(kmeans_results, key=lambda x: x['silhouette'])['k']
best_silhouette = max(kmeans_results, key=lambda x: x['silhouette'])['silhouette']
best_labels = max(kmeans_results, key=lambda x: x['silhouette'])['labels']

print(f"\nK ottimale: {best_k} (Silhouette: {best_silhouette:.3f})")

# =============================================================================
# STEP 7: CLUSTERING GERARCHICO
# =============================================================================

print("\n=== STEP 7: Clustering gerarchico ===")

# Test diversi linkage
linkage_methods = ['ward', 'complete', 'average']
hierarchical_results = []

for linkage in linkage_methods:
    for k in range(2, 5):
        hier = AgglomerativeClustering(n_clusters=k, linkage=linkage)
        labels = hier.fit_predict(X_scaled)
        
        silhouette = silhouette_score(X_scaled, labels)
        
        hierarchical_results.append({
            'linkage': linkage,
            'k': k,
            'silhouette': silhouette,
            'labels': labels
        })
        
        print(f"  {linkage} K={k}: Silhouette={silhouette:.3f}")

# Migliore hierarchical
best_hier = max(hierarchical_results, key=lambda x: x['silhouette'])
print(f"Migliore hierarchical: {best_hier['linkage']} K={best_hier['k']} (Sil: {best_hier['silhouette']:.3f})")

# =============================================================================
# STEP 8: PROFILING CLUSTER
# =============================================================================

print(f"\n=== STEP 8: Profiling cluster K-means K={best_k} ===")

# Aggiungi cluster labels al dataset
df_processed['cluster_kmeans'] = best_labels

# Profiling per cluster
for cluster_id in range(best_k):
    cluster_data = df_processed[df_processed['cluster_kmeans'] == cluster_id]
    n_obs = len(cluster_data)
    
    print(f"\n--- CLUSTER {cluster_id} ({n_obs} osservazioni, {n_obs/len(df_processed)*100:.1f}%) ---")
    
    # Demografia
    if 'eta' in cluster_data.columns:
        eta_denorm = cluster_data['eta'] * (70-18) + 18
        print(f"Età media: {eta_denorm.mean():.1f} anni")
    
    if 'genere_donna' in cluster_data.columns:
        donne_pct = cluster_data['genere_donna'].mean() * 100
        print(f"Donne: {donne_pct:.0f}%")
    
    if 'titolo_magistrale' in cluster_data.columns:
        magistrale_pct = cluster_data['titolo_magistrale'].mean() * 100
        print(f"Magistrale: {magistrale_pct:.0f}%")
    
    if 'occup_studente' in cluster_data.columns:
        studenti_pct = cluster_data['occup_studente'].mean() * 100
        print(f"Studenti: {studenti_pct:.0f}%")
    
    # Comportamenti
    if 'trasporto_sostenibile' in cluster_data.columns:
        trasporto_pct = cluster_data['trasporto_sostenibile'].mean() * 100
        print(f"Trasporto sostenibile: {trasporto_pct:.0f}%")
    
    # Likert sample
    likert_sample = ['lik7_q8', 'lik7_q19', 'lik7_q21']
    for var in likert_sample:
        if var in cluster_data.columns:
            print(f"{var}: {cluster_data[var].mean():.2f}")

# =============================================================================
# STEP 9: TEST ANOVA
# =============================================================================

print(f"\n=== STEP 9: Test ANOVA differenze tra cluster ===")

significant_vars = []

for var in clustering_vars_final:
    groups = [df_processed[df_processed['cluster_kmeans'] == i][var].dropna().values 
              for i in range(best_k)]
    
    # Rimuovi gruppi vuoti
    groups = [g for g in groups if len(g) > 0]
    
    if len(groups) >= 2 and all(len(g) > 1 for g in groups):
        try:
            f_stat, p_value = f_oneway(*groups)
            
            if p_value < 0.05:
                significant_vars.append(var)
                sig_level = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*"
                print(f"{var}: F={f_stat:.2f}, p={p_value:.4f} {sig_level}")
        except:
            continue

print(f"\nVariabili significative: {len(significant_vars)}/{len(clustering_vars_final)}")

# =============================================================================
# STEP 10: VISUALIZZAZIONI
# =============================================================================

print(f"\n=== STEP 10: Visualizzazioni ===")

# PCA per visualizzazione
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(15, 5))

# Plot 1: K-means
plt.subplot(1, 3, 1)
colors = ['red', 'blue', 'green', 'orange', 'purple']
for i in range(best_k):
    mask = best_labels == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i], 
               label=f'Cluster {i} (n={mask.sum()})', alpha=0.7)

plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title(f'K-means K={best_k}\nSilhouette: {best_silhouette:.3f}')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 2: Silhouette scores
plt.subplot(1, 3, 2)
k_values = [r['k'] for r in kmeans_results]
sil_values = [r['silhouette'] for r in kmeans_results]

plt.plot(k_values, sil_values, 'bo-', linewidth=2, markersize=8)
plt.xlabel('Numero Cluster (K)')
plt.ylabel('Silhouette Score')
plt.title('Selezione K Ottimale')
plt.grid(True, alpha=0.3)
plt.xticks(k_values)

# Plot 3: Distribuzione cluster
plt.subplot(1, 3, 3)
cluster_counts = pd.Series(best_labels).value_counts().sort_index()
plt.bar(range(len(cluster_counts)), cluster_counts.values, 
        color=colors[:len(cluster_counts)], alpha=0.7)
plt.xlabel('Cluster ID')
plt.ylabel('Numero Osservazioni')
plt.title('Distribuzione Cluster')
plt.xticks(range(len(cluster_counts)))

for i, count in enumerate(cluster_counts.values):
    plt.text(i, count + 1, str(count), ha='center')

plt.tight_layout()
plt.savefig('modulo1_clustering_base.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# STEP 11: EXPORT RISULTATI
# =============================================================================

print(f"\n=== STEP 11: Export risultati ===")

# Dataset processato per moduli successivi (formato richiesto dal modulo 5)
df_processed.to_excel('dataset_module4_python.xlsx', 
                     sheet_name='Dataset_Module4', 
                     index=False)

# Anche in CSV per backup
df_processed.to_csv('modulo1_dataset_processato.csv', index=False)

# Risultati clustering
clustering_summary = pd.DataFrame({
    'metodo': ['K-means'] * len(kmeans_results) + ['Hierarchical'] * len(hierarchical_results),
    'parametri': [f'K={r["k"]}' for r in kmeans_results] + 
                 [f'{r["linkage"]}_K={r["k"]}' for r in hierarchical_results],
    'silhouette': [r['silhouette'] for r in kmeans_results] + 
                  [r['silhouette'] for r in hierarchical_results]
})

clustering_summary.to_csv('modulo1_clustering_results.csv', index=False)

# Profili cluster
cluster_profiles = df_processed.groupby('cluster_kmeans')[clustering_vars_final].mean()
cluster_profiles.to_csv('modulo1_cluster_profiles.csv')

print(f"File salvati:")
print(f"- dataset_module4_python.xlsx: Dataset preprocessato per moduli successivi")
print(f"- modulo1_dataset_processato.csv: Dataset preprocessato (backup)")
print(f"- modulo1_clustering_results.csv: Risultati tutti i clustering testati")
print(f"- modulo1_cluster_profiles.csv: Profili medi per cluster")
print(f"- modulo1_clustering_base.png: Visualizzazioni principali")

# =============================================================================
# STEP 12: RIEPILOGO
# =============================================================================

print(f"\n=== RIEPILOGO MODULO 1 ===")
print(f"Dataset originale: {df_original.shape}")
print(f"Dataset processato: {df_processed.shape}")
print(f"Variabili clustering: {len(clustering_vars_final)}")
print(f"Metodo vincente: K-means K={best_k}")
print(f"Performance: Silhouette {best_silhouette:.3f}")
print(f"Variabili discriminanti: {len(significant_vars)}/{len(clustering_vars_final)}")

if best_silhouette > 0.3:
    quality = "Buona"
elif best_silhouette > 0.2:
    quality = "Accettabile"
else:
    quality = "Limitata"

print(f"Qualità clustering: {quality}")
print(f"\n=== MODULO 1 COMPLETATO ===")
print("Procedere con Modulo 2 per analisi avanzate")
