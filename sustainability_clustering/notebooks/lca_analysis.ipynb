# =============================================================================
# MODULO 7 LCA - VERSIONE SAS DATASET
# LCA su dataset preprocessato dai moduli SAS base
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import adjusted_rand_score, silhouette_score
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO 7 LCA - Dataset SAS Preprocessato ===")

# =============================================================================
# STEP 1: CARICA DATASET SAS PREPROCESSATO
# =============================================================================

# Prima prova a caricare i dati dai moduli precedenti
try:
    # Prova dai risultati MCA
    df_sas = pd.read_csv('modulo6_mca_results_complete.csv')
    print(f"Dataset MCA caricato: {df_sas.shape}")
    
    # Variabili clustering usate nei moduli SAS (dalle info project knowledge)
    sas_clustering_vars = [
        'eta', 'genere_donna', 'titolo_magistrale', 'occup_studente', 
        'geo_isole', 'reddito_medio_alto',
        'lik5_q15', 'lik5_q17', 'lik5_q27', 
        'trasporto_sostenibile', 'ostacolo_costi', 'resp_governi', 'partec_attiva',
        'lik7c_q8', 'lik7c_q13', 'lik7c_q19', 'lik7c_q20', 'lik7c_q21', 'lik7c_q23'
    ]
    
    # Filtra variabili esistenti
    existing_sas_vars = [var for var in sas_clustering_vars if var in df_sas.columns]
    print(f"Variabili SAS trovate: {len(existing_sas_vars)}")
    
    # Se non troviamo le variabili corrette, usa il dataset originale
    if len(existing_sas_vars) < 10:
        raise FileNotFoundError("Variabili SAS non trovate")
        
except:
    # Fallback al dataset originale
    print("Fallback a dataset originale...")
    df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                               sheet_name='Questionario Sostenibilità')
    
    # Simula preprocessing SAS (versione semplificata)
    df_sas = df_original.copy()
    
    # Encoding base
    df_sas['genere_donna'] = (df_sas['q2'] == 'Donna').astype(int)
    df_sas['titolo_magistrale'] = (df_sas['q3'] == 'Laurea Magistrale').astype(int)
    df_sas['occup_studente'] = (df_sas['q4'] == 'Studente/ Studentessa').astype(int)
    df_sas['geo_isole'] = (df_sas['q5'] == 'Isole').astype(int)
    df_sas['reddito_medio_alto'] = df_sas['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
    df_sas['trasporto_sostenibile'] = df_sas['q22'].isin(['Bicicletta o a piedi', 'Mezzo privato (ibrido/elettrico)']).astype(int)
    df_sas['ostacolo_costi'] = (df_sas['q25'] == 'I costi più elevati').astype(int)
    df_sas['resp_governi'] = (df_sas['q26'] == 'I governi e le istituzioni internazionali').astype(int)
    df_sas['partec_attiva'] = df_sas['q28'].isin(['Sì, più di una volta', 'Sì, spesso']).astype(int)
    
    # Età normalizzata
    df_sas['eta'] = (df_sas['q1'] - 18) / (70 - 18)
    
    # Likert 1-5
    for var in ['q15', 'q17', 'q27']:
        if var in df_sas.columns:
            mapping = {'Mai': 1, 'Raramente': 2, 'Qualche volta': 3, 'Spesso': 4, 'Sempre': 5}
            df_sas[f'lik5_{var}'] = df_sas[var].map(mapping).fillna(3)
    
    # Likert 1-7 (senza size effect per ora)
    likert_7_vars = ['q8', 'q13', 'q19', 'q20', 'q21', 'q23']
    for var in likert_7_vars:
        if var in df_sas.columns:
            df_sas[f'lik7c_{var}'] = df_sas[var].fillna(4)
    
    existing_sas_vars = [
        'eta', 'genere_donna', 'titolo_magistrale', 'occup_studente', 
        'geo_isole', 'reddito_medio_alto',
        'lik5_q15', 'lik5_q17', 'lik5_q27',
        'trasporto_sostenibile', 'ostacolo_costi', 'resp_governi', 'partec_attiva',
        'lik7c_q8', 'lik7c_q13', 'lik7c_q19', 'lik7c_q20', 'lik7c_q21', 'lik7c_q23'
    ]
    
    print(f"Dataset preprocessato creato: {df_sas.shape}")

# =============================================================================
# STEP 2: PREPARAZIONE DATASET LCA
# =============================================================================

# Dataset per LCA
X_lca = df_sas[existing_sas_vars].copy()

# Rimuovi missing
X_lca = X_lca.dropna()

print(f"Dataset LCA finale: {X_lca.shape}")
print(f"Missing values: {X_lca.isnull().sum().sum()}")

# Standardizzazione
scaler = StandardScaler()
X_lca_scaled = scaler.fit_transform(X_lca)

print("Dataset standardizzato per LCA")

# =============================================================================
# STEP 3: SELEZIONE NUMERO CLASSI
# =============================================================================

print(f"\n=== Selezione numero classi latenti ===")

n_classes_range = range(2, 7)
lca_results = []

for n_classes in n_classes_range:
    print(f"Testing K={n_classes}...", end=" ")
    
    lca_model = GaussianMixture(
        n_components=n_classes,
        covariance_type='diag',
        random_state=42,
        max_iter=200,
        n_init=5
    )
    
    try:
        lca_model.fit(X_lca_scaled)
        
        bic = lca_model.bic(X_lca_scaled)
        aic = lca_model.aic(X_lca_scaled)
        
        labels = lca_model.predict(X_lca_scaled)
        n_unique = len(np.unique(labels))
        
        if n_unique == n_classes and n_unique > 1:
            sil = silhouette_score(X_lca_scaled, labels)
        else:
            sil = 0.0
        
        lca_results.append({
            'n_classes': n_classes,
            'bic': bic,
            'aic': aic,
            'silhouette': sil,
            'converged': lca_model.converged_
        })
        
        print(f"BIC={bic:.0f}, AIC={aic:.0f}, Sil={sil:.3f}")
        
    except Exception as e:
        print(f"Errore: {e}")

# Trova K ottimale
lca_df = pd.DataFrame(lca_results)
optimal_k_bic = lca_df.loc[lca_df['bic'].idxmin(), 'n_classes']
optimal_k_aic = lca_df.loc[lca_df['aic'].idxmin(), 'n_classes']
optimal_k_sil = lca_df.loc[lca_df['silhouette'].idxmax(), 'n_classes']

print(f"\nK ottimale BIC: {optimal_k_bic}")
print(f"K ottimale AIC: {optimal_k_aic}")
print(f"K ottimale Silhouette: {optimal_k_sil}")

# Scelta finale
optimal_k = optimal_k_bic
print(f"K scelto: {optimal_k}")

# =============================================================================
# STEP 4: MODELLO FINALE
# =============================================================================

print(f"\n=== Modello LCA finale K={optimal_k} ===")

lca_final = GaussianMixture(
    n_components=optimal_k,
    covariance_type='diag',
    random_state=42,
    max_iter=300,
    n_init=10
)

lca_final.fit(X_lca_scaled)

# Predizioni
lca_labels = lca_final.predict(X_lca_scaled)
lca_probs = lca_final.predict_proba(X_lca_scaled)

print(f"Convergenza: {lca_final.converged_}")
print(f"Log-likelihood: {lca_final.score(X_lca_scaled):.3f}")

# Distribuzione classi
print(f"\nDistribuzione classi:")
class_counts = pd.Series(lca_labels).value_counts().sort_index()
for i, count in class_counts.items():
    pct = count / len(lca_labels) * 100
    print(f"Classe {i}: {count} obs ({pct:.1f}%)")

# Qualità membership
max_probs = np.max(lca_probs, axis=1)
print(f"\nProbabilità membership:")
print(f"Media: {np.mean(max_probs):.3f}")
print(f"Incerte (<0.7): {np.sum(max_probs < 0.7)} ({np.sum(max_probs < 0.7)/len(max_probs)*100:.1f}%)")

# =============================================================================
# STEP 5: PROFILING CLASSI
# =============================================================================

print(f"\n=== Profiling {optimal_k} classi ===")

# Aggiungi risultati al dataframe
df_results = X_lca.copy()
df_results['lca_class'] = lca_labels
df_results['lca_prob'] = max_probs

# Profili per classe
for class_id in range(optimal_k):
    class_data = df_results[df_results['lca_class'] == class_id]
    n_obs = len(class_data)
    
    print(f"\n--- CLASSE {class_id} ({n_obs} obs, {n_obs/len(df_results)*100:.1f}%) ---")
    
    # Demografia
    if 'genere_donna' in class_data.columns:
        genere_pct = class_data['genere_donna'].mean() * 100
        print(f"Donne: {genere_pct:.0f}%")
    
    if 'titolo_magistrale' in class_data.columns:
        magistrale_pct = class_data['titolo_magistrale'].mean() * 100
        print(f"Magistrale: {magistrale_pct:.0f}%")
    
    if 'occup_studente' in class_data.columns:
        studenti_pct = class_data['occup_studente'].mean() * 100
        print(f"Studenti: {studenti_pct:.0f}%")
    
    # Comportamenti
    if 'trasporto_sostenibile' in class_data.columns:
        trasporto_pct = class_data['trasporto_sostenibile'].mean() * 100
        print(f"Trasporto sostenibile: {trasporto_pct:.0f}%")
    
    # Atteggiamenti (campione)
    likert_vars = ['lik7c_q8', 'lik7c_q19', 'lik7c_q21']
    likert_means = []
    for var in likert_vars:
        if var in class_data.columns:
            mean_val = class_data[var].mean()
            likert_means.append(mean_val)
            print(f"{var}: {mean_val:.1f}")

# =============================================================================
# STEP 6: CONFRONTI CON ALTRI METODI
# =============================================================================

print(f"\n=== Confronti con altri metodi ===")

# Silhouette LCA
if len(np.unique(lca_labels)) > 1:
    lca_silhouette = silhouette_score(X_lca_scaled, lca_labels)
else:
    lca_silhouette = 0.0

print(f"Silhouette LCA: {lca_silhouette:.3f}")

# Confronto con MCA se disponibile
try:
    mca_results = pd.read_csv('modulo6_mca_results_complete.csv')
    if len(mca_results) == len(df_results):
        ari_mca = adjusted_rand_score(lca_labels, mca_results['mca_cluster'])
        print(f"ARI vs MCA: {ari_mca:.3f}")
        
        print("\nCrosstab LCA vs MCA:")
        crosstab = pd.crosstab(lca_labels, mca_results['mca_cluster'], margins=True)
        print(crosstab)
except:
    print("MCA non disponibile per confronto")

# Confronto con Hierarchical se disponibile  
try:
    hier_results = pd.read_csv('modulo5_hierarchical_results.csv')
    if len(hier_results) == len(df_results):
        ari_hier = adjusted_rand_score(lca_labels, hier_results['hier_cluster'])
        print(f"ARI vs Hierarchical: {ari_hier:.3f}")
except:
    print("Hierarchical non disponibile per confronto")

# =============================================================================
# STEP 7: VISUALIZZAZIONE
# =============================================================================

print(f"\n=== Visualizzazioni ===")

# PCA per visualizzazione
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_lca_scaled)

plt.figure(figsize=(12, 8))
colors = plt.cm.Set1(np.linspace(0, 1, optimal_k))

for i in range(optimal_k):
    mask = lca_labels == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
               c=[colors[i]], alpha=0.7, s=50,
               label=f'Classe {i} (n={mask.sum()})')

plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title(f'LCA {optimal_k} Classi - Dataset SAS Preprocessato')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('modulo7_lca_sas_visualization.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# STEP 8: EXPORT RISULTATI
# =============================================================================

# Export risultati
df_export = df_results.copy()
for i in range(optimal_k):
    df_export[f'lca_prob_class_{i}'] = lca_probs[:, i]

df_export.to_csv('modulo7_lca_sas_results.csv', index=False)

print(f"\n=== RIEPILOGO FINALE ===")
print(f"Dataset: SAS preprocessato ({X_lca.shape})")
print(f"Classi: {optimal_k}")
print(f"Silhouette: {lca_silhouette:.3f}")
print(f"Probabilità media: {np.mean(max_probs):.3f}")
print(f"Convergenza: {lca_final.converged_}")
