# =============================================================================
# MODULO ENSEMBLE CLUSTERING
# Combinazione di multiple metodologie per clustering robusto
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
from scipy.stats import mode
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO ENSEMBLE CLUSTERING ===")

# =============================================================================
# STEP 1: PREPARAZIONE DATASET E METODI
# =============================================================================

print("\n=== STEP 1: Preparazione per ensemble clustering ===")

# Carica dataset con preprocessing ottimale dalla sensitivity analysis
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

# Usa configurazione demografica ottimale dalla sensitivity analysis
def prepare_demographic_data(df):
    df_encoded = df.copy()
    
    # Demografia encoding (migliore dalla sensitivity)
    df_encoded['eta_norm'] = (df_encoded['q1'] - 18) / (70 - 18)
    df_encoded['genere_donna'] = (df_encoded['q2'] == 'Donna').astype(int)
    df_encoded['titolo_magistrale'] = (df_encoded['q3'] == 'Laurea Magistrale').astype(int)
    df_encoded['titolo_postlaurea'] = (df_encoded['q3'] == 'Formazione Post Laurea (Master/Dottorato)').astype(int)
    df_encoded['occup_studente'] = (df_encoded['q4'] == 'Studente/ Studentessa').astype(int)
    df_encoded['occup_privato'] = (df_encoded['q4'] == 'Lavoro dipendente privato').astype(int)
    df_encoded['geo_nord'] = df_encoded['q5'].isin(['Nord Est', 'Nord Ovest']).astype(int)
    df_encoded['geo_centro'] = (df_encoded['q5'] == 'Centro').astype(int)
    df_encoded['reddito_alto'] = df_encoded['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
    df_encoded['reddito_basso'] = (df_encoded['q6'] == 'Meno di 15000').astype(int)
    
    demo_vars = ['eta_norm', 'genere_donna', 'titolo_magistrale', 'titolo_postlaurea', 
                 'occup_studente', 'occup_privato', 'geo_nord', 'geo_centro', 
                 'reddito_alto', 'reddito_basso']
    
    return df_encoded[demo_vars].fillna(df_encoded[demo_vars].mean())

# Prepara dataset
data_demo = prepare_demographic_data(df_original)
print(f"Dataset demografico: {data_demo.shape}")

# Preprocessing ottimale dalla sensitivity (RobustScaler + keep_all)
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
data_scaled = scaler.fit_transform(data_demo)

print("Preprocessing completato con RobustScaler")

# =============================================================================
# STEP 2: DEFINIZIONE METODI ENSEMBLE
# =============================================================================

print("\n=== STEP 2: Definizione metodi per ensemble ===")

# Ensemble methods - diversi algoritmi e parametri
ensemble_methods = {
    # K-Means varianti
    'KMeans_2': KMeans(n_clusters=2, random_state=42, n_init=10),
    'KMeans_3': KMeans(n_clusters=3, random_state=42, n_init=10),
    'KMeans_4': KMeans(n_clusters=4, random_state=42, n_init=10),
    
    # Hierarchical varianti
    'Hier_Ward_2': AgglomerativeClustering(n_clusters=2, linkage='ward'),
    'Hier_Ward_3': AgglomerativeClustering(n_clusters=3, linkage='ward'),
    'Hier_Average_2': AgglomerativeClustering(n_clusters=2, linkage='average'),
    'Hier_Average_3': AgglomerativeClustering(n_clusters=3, linkage='average'),
    'Hier_Complete_2': AgglomerativeClustering(n_clusters=2, linkage='complete'),
    
    # Gaussian Mixture
    'GMM_2': GaussianMixture(n_components=2, random_state=42, max_iter=100),
    'GMM_3': GaussianMixture(n_components=3, random_state=42, max_iter=100),
    'GMM_4': GaussianMixture(n_components=4, random_state=42, max_iter=100),
    
    # Spectral Clustering
    'Spectral_2': SpectralClustering(n_clusters=2, random_state=42, affinity='rbf'),
    'Spectral_3': SpectralClustering(n_clusters=3, random_state=42, affinity='rbf'),
    
    # DBSCAN variants (se funziona)
    'DBSCAN_05': DBSCAN(eps=0.5, min_samples=5),
    'DBSCAN_07': DBSCAN(eps=0.7, min_samples=10),
}

print(f"Metodi ensemble definiti: {len(ensemble_methods)}")

# =============================================================================
# STEP 3: ESECUZIONE ENSEMBLE
# =============================================================================

print("\n=== STEP 3: Esecuzione metodi ensemble ===")

ensemble_results = {}
ensemble_labels = {}
silhouette_scores = {}

for method_name, clusterer in ensemble_methods.items():
    try:
        print(f"  Eseguendo {method_name}...", end=" ")
        
        labels = clusterer.fit_predict(data_scaled)
        n_clusters = len(np.unique(labels))
        
        # Verifica clustering valido
        if n_clusters > 1 and n_clusters < len(data_scaled) * 0.8:  # Non troppo frammentato
            silhouette = silhouette_score(data_scaled, labels)
            
            ensemble_results[method_name] = {
                'labels': labels,
                'n_clusters': n_clusters,
                'silhouette': silhouette,
                'method_type': method_name.split('_')[0],
                'valid': True
            }
            
            ensemble_labels[method_name] = labels
            silhouette_scores[method_name] = silhouette
            
            print(f"OK - {n_clusters} cluster, sil={silhouette:.3f}")
            
        else:
            print(f"SKIP - {n_clusters} cluster non validi")
            ensemble_results[method_name] = {'valid': False}
    
    except Exception as e:
        print(f"ERROR - {str(e)[:50]}")
        ensemble_results[method_name] = {'valid': False}

valid_methods = [name for name, result in ensemble_results.items() if result.get('valid', False)]
print(f"\nMetodi validi: {len(valid_methods)}/{len(ensemble_methods)}")

# =============================================================================
# STEP 4: CONSENSUS CLUSTERING
# =============================================================================

print("\n=== STEP 4: Consensus clustering ===")

if len(valid_methods) >= 3:
    # Costruisci consensus matrix
    n_samples = len(data_scaled)
    consensus_matrix = np.zeros((n_samples, n_samples))
    
    print("Costruendo consensus matrix...")
    
    for method_name in valid_methods:
        labels = ensemble_labels[method_name]
        
        # Aggiorna consensus matrix
        for i in range(n_samples):
            for j in range(i+1, n_samples):
                if labels[i] == labels[j]:
                    consensus_matrix[i, j] += 1
                    consensus_matrix[j, i] += 1
    
    # Normalizza per numero metodi
    consensus_matrix = consensus_matrix / len(valid_methods)
    
    # Statistiche consensus
    consensus_mean = np.mean(consensus_matrix[np.triu_indices_from(consensus_matrix, k=1)])
    consensus_std = np.std(consensus_matrix[np.triu_indices_from(consensus_matrix, k=1)])
    
    print(f"Consensus score: {consensus_mean:.3f} ± {consensus_std:.3f}")
    
    # Clustering finale da consensus matrix
    distance_matrix = 1 - consensus_matrix
    np.fill_diagonal(distance_matrix, 0)
    
    # Test diversi K per consensus
    consensus_results = {}
    
    for k in [2, 3, 4]:
        try:
            consensus_clusterer = AgglomerativeClustering(
                n_clusters=k, 
                metric='precomputed',
                linkage='average'
            )
            
            consensus_labels = consensus_clusterer.fit_predict(distance_matrix)
            consensus_sil = silhouette_score(data_scaled, consensus_labels)
            
            consensus_results[k] = {
                'labels': consensus_labels,
                'silhouette': consensus_sil
            }
            
            print(f"Consensus K={k}: silhouette={consensus_sil:.3f}")
            
        except Exception as e:
            print(f"Consensus K={k}: errore")
    
    # Migliore consensus
    if consensus_results:
        best_k = max(consensus_results.keys(), key=lambda k: consensus_results[k]['silhouette'])
        best_consensus = consensus_results[best_k]
        print(f"Migliore consensus: K={best_k}, silhouette={best_consensus['silhouette']:.3f}")
    else:
        best_k = None
        best_consensus = None

else:
    print("Troppo pochi metodi validi per consensus clustering")
    consensus_matrix = None
    best_consensus = None
    best_k = None

# =============================================================================
# STEP 5: VOTING ENSEMBLE
# =============================================================================

print("\n=== STEP 5: Voting ensemble ===")

if len(valid_methods) >= 3:
    # Majority voting per ogni K
    voting_results = {}
    
    for target_k in [2, 3]:
        print(f"\nVoting ensemble per K={target_k}:")
        
        # Filtra metodi con K target
        k_methods = [name for name in valid_methods 
                    if ensemble_results[name]['n_clusters'] == target_k]
        
        if len(k_methods) >= 2:
            print(f"  Metodi disponibili: {len(k_methods)} ({k_methods[:3]}...)")
            
            # Matrice votazioni
            votes_matrix = np.zeros((len(data_scaled), len(k_methods)))
            
            for i, method_name in enumerate(k_methods):
                votes_matrix[:, i] = ensemble_labels[method_name]
            
            # Hard voting - cluster più frequente per ogni punto
            voting_labels = []
            vote_confidence = []
            
            for i in range(len(data_scaled)):
                point_votes = votes_matrix[i, :]
                
                # Trova cluster più votato
                vote_counts = Counter(point_votes)
                most_common_cluster, max_votes = vote_counts.most_common(1)[0]
                confidence = max_votes / len(k_methods)
                
                voting_labels.append(int(most_common_cluster))
                vote_confidence.append(confidence)
            
            voting_labels = np.array(voting_labels)
            vote_confidence = np.array(vote_confidence)
            
            # Ri-mappa cluster labels per consistenza (0, 1, ...)
            unique_labels = np.unique(voting_labels)
            label_map = {old: new for new, old in enumerate(unique_labels)}
            voting_labels_remapped = np.array([label_map[label] for label in voting_labels])
            
            # Calcola metriche
            if len(np.unique(voting_labels_remapped)) > 1:
                voting_sil = silhouette_score(data_scaled, voting_labels_remapped)
                avg_confidence = np.mean(vote_confidence)
                
                voting_results[target_k] = {
                    'labels': voting_labels_remapped,
                    'silhouette': voting_sil,
                    'confidence': avg_confidence,
                    'n_methods': len(k_methods)
                }
                
                print(f"  Silhouette: {voting_sil:.3f}")
                print(f"  Confidence media: {avg_confidence:.3f}")
                print(f"  Cluster effettivi: {len(np.unique(voting_labels_remapped))}")
        else:
            print(f"  Troppo pochi metodi per K={target_k}")
    
    # Migliore voting
    if voting_results:
        best_voting_k = max(voting_results.keys(), key=lambda k: voting_results[k]['silhouette'])
        best_voting = voting_results[best_voting_k]
        print(f"\nMigliore voting: K={best_voting_k}, silhouette={best_voting['silhouette']:.3f}")
    else:
        best_voting_k = None
        best_voting = None

else:
    print("Troppo pochi metodi validi per voting ensemble")
    voting_results = {}
    best_voting = None
    best_voting_k = None

# =============================================================================
# STEP 6: ANALISI COMPARATIVA ENSEMBLE
# =============================================================================

print("\n=== STEP 6: Analisi comparativa ensemble ===")

# Confronta tutti i risultati
comparison_results = []

# Singoli metodi
for method_name in valid_methods:
    result = ensemble_results[method_name]
    comparison_results.append({
        'Method': method_name,
        'Type': 'Single',
        'K': result['n_clusters'],
        'Silhouette': result['silhouette'],
        'Approach': result['method_type']
    })

# Consensus
if best_consensus:
    comparison_results.append({
        'Method': f'Consensus_K{best_k}',
        'Type': 'Consensus',
        'K': best_k,
        'Silhouette': best_consensus['silhouette'],
        'Approach': f'{len(valid_methods)} methods'
    })

# Voting
if best_voting:
    comparison_results.append({
        'Method': f'Voting_K{best_voting_k}',
        'Type': 'Voting',
        'K': best_voting_k,
        'Silhouette': best_voting['silhouette'],
        'Approach': f'{best_voting["n_methods"]} methods'
    })

comparison_df = pd.DataFrame(comparison_results)

if len(comparison_df) > 0:
    print("RANKING ENSEMBLE RESULTS:")
    top_results = comparison_df.nlargest(10, 'Silhouette')
    
    for i, (_, row) in enumerate(top_results.iterrows(), 1):
        print(f"{i:2d}. {row['Method']:<20} ({row['Type']:<9}): {row['Silhouette']:.3f} - K={row['K']}")
    
    # Analisi per tipo
    print(f"\nPERFORMANCE PER TIPO:")
    type_stats = comparison_df.groupby('Type')['Silhouette'].agg(['mean', 'max', 'count'])
    for type_name, stats in type_stats.iterrows():
        print(f"  {type_name}: μ={stats['mean']:.3f}, max={stats['max']:.3f}, n={stats['count']}")
    
    # Stabilità ensemble vs singoli metodi
    single_methods = comparison_df[comparison_df['Type'] == 'Single']['Silhouette']
    ensemble_methods = comparison_df[comparison_df['Type'].isin(['Consensus', 'Voting'])]['Silhouette']
    
    if len(single_methods) > 1:
        single_cv = single_methods.std() / single_methods.mean()
        print(f"\nSTABILITÀ:")
        print(f"  Single methods CV: {single_cv:.3f}")
        
        if len(ensemble_methods) > 0:
            ensemble_cv = ensemble_methods.std() / ensemble_methods.mean() if len(ensemble_methods) > 1 else 0
            print(f"  Ensemble methods CV: {ensemble_cv:.3f}")
            
            if ensemble_cv < single_cv:
                print("  ✓ Ensemble più stabile dei singoli metodi")
            else:
                print("  ⚠ Ensemble non più stabile dei singoli metodi")
else:
    print("Nessun risultato valido per comparazione")

# =============================================================================
# STEP 7: RIEPILOGO ENSEMBLE
# =============================================================================

print(f"\n=== RIEPILOGO ENSEMBLE CLUSTERING ===")

if len(valid_methods) > 0:
    print(f"Metodi validi testati: {len(valid_methods)}")
    print(f"Range silhouette: {min(silhouette_scores.values()):.3f} - {max(silhouette_scores.values()):.3f}")
    
    # Vincitore assoluto
    winner = max(comparison_results, key=lambda x: x['Silhouette'])
    print(f"\nVINCITORE ENSEMBLE: {winner['Method']}")
    print(f"  Tipo: {winner['Type']}")
    print(f"  Silhouette: {winner['Silhouette']:.3f}")
    print(f"  K cluster: {winner['K']}")
    
    # Robustezza
    all_silhouettes = [r['Silhouette'] for r in comparison_results]
    overall_cv = np.std(all_silhouettes) / np.mean(all_silhouettes)
    
    if overall_cv < 0.2:
        stability = "Alta robustezza"
    elif overall_cv < 0.4:
        stability = "Media robustezza"
    else:
        stability = "Bassa robustezza"
    
    print(f"\nROBUSTEZZA ENSEMBLE: {stability} (CV={overall_cv:.3f})")
    
    # Consenso metodologico
    k_counts = Counter([r['K'] for r in comparison_results])
    dominant_k = k_counts.most_common(1)[0][0]
    k_consensus = k_counts[dominant_k] / len(comparison_results)
    
    print(f"CONSENSO K: K={dominant_k} in {k_consensus*100:.1f}% dei metodi")
    
else:
    print("ERRORE: Nessun metodo ensemble valido")

print("\n=== MODULO ENSEMBLE CLUSTERING COMPLETATO ===")
