# =============================================================================
# MODULO CROSS-DATASET 1: K-MEANS SU TUTTI I DATASET
# Confronto sistematico performance K-Means con diversi preprocessing
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, adjusted_rand_score
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO CROSS-DATASET 1: K-MEANS COMPARISON ===")

# =============================================================================
# STEP 1: PREPARAZIONE TUTTI I DATASET
# =============================================================================

print("\n=== STEP 1: Caricamento e preparazione dataset ===")

datasets = {}
dataset_info = {}

# 1. Dataset Originale
print("Preparando Dataset Originale...")
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

# Preprocessing base per originale
df_orig_processed = df_original.copy()

# Encoding demografico base
df_orig_processed['genere_donna'] = (df_orig_processed['q2'] == 'Donna').astype(int)
df_orig_processed['titolo_magistrale'] = (df_orig_processed['q3'] == 'Laurea Magistrale').astype(int)
df_orig_processed['occup_studente'] = (df_orig_processed['q4'] == 'Studente/ Studentessa').astype(int)
df_orig_processed['eta_norm'] = (df_orig_processed['q1'] - 18) / (70 - 18)

# Likert selection
likert_vars = ['q7', 'q8', 'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 
               'q16', 'q18', 'q19', 'q20', 'q21', 'q23', 'q24', 'q29']
existing_likert = [var for var in likert_vars if var in df_orig_processed.columns]

# Variabili finali per originale
orig_vars = ['eta_norm', 'genere_donna', 'titolo_magistrale', 'occup_studente'] + existing_likert
orig_data = df_orig_processed[orig_vars].fillna(df_orig_processed[orig_vars].mean())

datasets['Original'] = orig_data
dataset_info['Original'] = f"Shape: {orig_data.shape}, Variables: Mixed (demo + likert)"

# 2. Dataset Likert Puro (dal Modulo 8)
print("Preparando Dataset Likert Puro...")
df_likert_pure = df_original[existing_likert].fillna(df_original[existing_likert].mean())

datasets['Likert_Pure'] = df_likert_pure
dataset_info['Likert_Pure'] = f"Shape: {df_likert_pure.shape}, Variables: Likert 1-7 only"

# 3. Factor Scores (se disponibili dal Modulo 8)
try:
    factor_results = pd.read_csv('modulo8_factor_analysis_results.csv')
    factor_cols = ['Factor_1', 'Factor_2', 'Factor_3']
    existing_factors = [col for col in factor_cols if col in factor_results.columns]
    
    if existing_factors:
        factor_data = factor_results[existing_factors]
        datasets['Factor_Scores'] = factor_data
        dataset_info['Factor_Scores'] = f"Shape: {factor_data.shape}, Variables: FA factors"
        print(f"Factor Scores caricati: {existing_factors}")
    else:
        print("Factor Scores non trovati nei risultati Modulo 8")
        
except FileNotFoundError:
    print("File Factor Analysis non trovato")

# 4. MCA Coordinates (se disponibili dal Modulo 6)
try:
    mca_results = pd.read_csv('modulo6_mca_coordinates.csv')
    mca_cols = ['Dim_1', 'Dim_2', 'Dim_3']
    existing_mca = [col for col in mca_cols if col in mca_results.columns]
    
    if existing_mca:
        mca_data = mca_results[existing_mca]
        datasets['MCA_Coords'] = mca_data
        dataset_info['MCA_Coords'] = f"Shape: {mca_data.shape}, Variables: MCA coordinates"
        print(f"MCA Coordinates caricati: {existing_mca}")
    else:
        print("MCA Coordinates non trovati nei risultati Modulo 6")
        
except FileNotFoundError:
    print("File MCA non trovato")

# 5. Dataset SAS Simulato (preprocessing avanzato)
print("Preparando Dataset SAS-style...")
df_sas_style = df_original.copy()

# Encoding completo SAS-style
sas_vars = []

# Demografia
df_sas_style['eta'] = (df_sas_style['q1'] - 18) / (70 - 18)
df_sas_style['genere_donna'] = (df_sas_style['q2'] == 'Donna').astype(int)
df_sas_style['titolo_magistrale'] = (df_sas_style['q3'] == 'Laurea Magistrale').astype(int)
df_sas_style['occup_studente'] = (df_sas_style['q4'] == 'Studente/ Studentessa').astype(int)
df_sas_style['geo_isole'] = (df_sas_style['q5'] == 'Isole').astype(int)
df_sas_style['reddito_alto'] = df_sas_style['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)

# Comportamenti
freq_mapping = {'Mai': 1, 'Raramente': 2, 'Qualche volta': 3, 'Spesso': 4, 'Sempre': 5}
for var in ['q15', 'q17', 'q27']:
    if var in df_sas_style.columns:
        df_sas_style[f'freq_{var}'] = df_sas_style[var].map(freq_mapping).fillna(3)
        sas_vars.append(f'freq_{var}')

df_sas_style['trasporto_sost'] = df_sas_style['q22'].isin(['Bicicletta o a piedi', 'Mezzo privato (ibrido/elettrico)']).astype(int)
df_sas_style['ostacolo_costi'] = (df_sas_style['q25'] == 'I costi più elevati').astype(int)

# Likert con size effect correction (approssimazione)
for var in existing_likert:
    if var in df_sas_style.columns:
        # Size effect removal semplificato
        individual_mean = df_sas_style[existing_likert].mean(axis=1)
        df_sas_style[f'{var}_corrected'] = df_sas_style[var] - individual_mean + df_sas_style[var].mean()
        sas_vars.append(f'{var}_corrected')

sas_base_vars = ['eta', 'genere_donna', 'titolo_magistrale', 'occup_studente', 'geo_isole', 'reddito_alto', 
                'trasporto_sost', 'ostacolo_costi']
sas_final_vars = sas_base_vars + sas_vars

sas_data = df_sas_style[sas_final_vars].fillna(df_sas_style[sas_final_vars].mean())

datasets['SAS_Processed'] = sas_data
dataset_info['SAS_Processed'] = f"Shape: {sas_data.shape}, Variables: Feature-engineered"

print(f"\nDataset preparati: {len(datasets)}")
for name, info in dataset_info.items():
    print(f"  {name}: {info}")

# =============================================================================
# STEP 2: K-MEANS SU OGNI DATASET
# =============================================================================

print(f"\n=== STEP 2: K-Means cross-dataset comparison ===")

k_range = range(2, 8)
results = []

for dataset_name, data in datasets.items():
    print(f"\nTesting {dataset_name}...")
    
    # Standardizzazione
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    
    # Test K-means per diversi K
    for k in k_range:
        try:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(data_scaled)
            
            # Metriche
            if len(np.unique(labels)) == k:
                silhouette = silhouette_score(data_scaled, labels)
                calinski_harabasz = calinski_harabasz_score(data_scaled, labels)
                inertia = kmeans.inertia_
                
                results.append({
                    'Dataset': dataset_name,
                    'K': k,
                    'Silhouette': silhouette,
                    'Calinski_Harabasz': calinski_harabasz,
                    'Inertia': inertia,
                    'N_Features': data.shape[1],
                    'N_Samples': data.shape[0]
                })
                
                print(f"  K={k}: Sil={silhouette:.3f}, CH={calinski_harabasz:.1f}")
            else:
                print(f"  K={k}: Solo {len(np.unique(labels))} cluster effettivi")
                
        except Exception as e:
            print(f"  K={k}: Errore - {e}")

results_df = pd.DataFrame(results)
print(f"\nRisultati totali: {len(results_df)} combinazioni testate")

# =============================================================================
# STEP 3: ANALISI RISULTATI
# =============================================================================

print(f"\n=== STEP 3: Analisi risultati K-Means ===")

# Migliori risultati per dataset
print("Migliori Silhouette per dataset:")
for dataset_name in datasets.keys():
    dataset_results = results_df[results_df['Dataset'] == dataset_name]
    if len(dataset_results) > 0:
        best_row = dataset_results.loc[dataset_results['Silhouette'].idxmax()]
        print(f"  {dataset_name}: K={best_row['K']}, Sil={best_row['Silhouette']:.3f}")

# Ranking complessivo
print(f"\nTop 10 risultati K-Means (tutti dataset):")
top_results = results_df.nlargest(10, 'Silhouette')
for _, row in top_results.iterrows():
    print(f"  {row['Dataset']} K={row['K']}: Sil={row['Silhouette']:.3f}, CH={row['Calinski_Harabasz']:.0f}")

print("=== Continua con visualizzazioni ===")
