# =============================================================================
# MODULO TEST FUZZY K-MEANS CLUSTERING  
# =============================================================================


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO SOM - SELF-ORGANIZING MAPS===")

# Per ora usiamo una implementazione SOM semplificata senza minisom
# che potrebbe non essere disponibile

def simple_som_implementation(data, som_width=6, som_height=6, learning_rate=0.5, iterations=500):
    """
    Implementazione SOM semplificata senza dipendenze esterne
    """
    n_samples, n_features = data.shape
    
    # Inizializza pesi random
    np.random.seed(42)
    weights = np.random.random((som_width, som_height, n_features))
    
    print(f"Inizializzando SOM {som_width}x{som_height} con {iterations} iterazioni...")
    
    # Training
    for iteration in range(iterations):
        # Decadimento learning rate e neighborhood radius
        current_lr = learning_rate * (1 - iteration/iterations)
        current_radius = max(1, som_width * (1 - iteration/iterations))
        
        # Per ogni sample
        for sample in data:
            # Trova BMU (Best Matching Unit)
            distances = np.sum((weights - sample)**2, axis=2)
            bmu_idx = np.unravel_index(np.argmin(distances), distances.shape)
            
            # Aggiorna pesi nel neighborhood
            for i in range(som_width):
                for j in range(som_height):
                    # Distanza dal BMU
                    dist_to_bmu = np.sqrt((i - bmu_idx[0])**2 + (j - bmu_idx[1])**2)
                    
                    if dist_to_bmu <= current_radius:
                        # Influence function (Gaussiana)
                        influence = np.exp(-(dist_to_bmu**2) / (2 * (current_radius**2)))
                        
                        # Update rule
                        weights[i, j] += current_lr * influence * (sample - weights[i, j])
        
        if (iteration + 1) % 100 == 0:
            print(f"  Iterazione {iteration + 1}/{iterations} completata")
    
    return weights

def find_bmu_for_samples(data, som_weights):
    """Trova Best Matching Unit per ogni sample"""
    som_width, som_height = som_weights.shape[0], som_weights.shape[1]
    bmu_coords = []
    
    for sample in data:
        distances = np.sum((som_weights - sample)**2, axis=2)
        bmu_idx = np.unravel_index(np.argmin(distances), distances.shape)
        bmu_coords.append(bmu_idx)
    
    return np.array(bmu_coords)

def create_distance_map(som_weights):
    """Crea U-matrix (distanze tra neuroni adiacenti)"""
    som_width, som_height = som_weights.shape[0], som_weights.shape[1]
    u_matrix = np.zeros((som_width, som_height))
    
    for i in range(som_width):
        for j in range(som_height):
            neighbors = []
            # Controllo neuroni adiacenti
            for di in [-1, 0, 1]:
                for dj in [-1, 0, 1]:
                    ni, nj = i + di, j + dj
                    if 0 <= ni < som_width and 0 <= nj < som_height and (di != 0 or dj != 0):
                        distance = np.linalg.norm(som_weights[i, j] - som_weights[ni, nj])
                        neighbors.append(distance)
            
            u_matrix[i, j] = np.mean(neighbors) if neighbors else 0
    
    return u_matrix

# Preparazione dataset (stesso del modulo precedente)
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

def prepare_som_data():
    df_encoded = df_original.copy()
    
    df_encoded['eta_norm'] = (df_encoded['q1'] - 18) / (70 - 18)
    df_encoded['genere_donna'] = (df_encoded['q2'] == 'Donna').astype(int)
    df_encoded['titolo_magistrale'] = (df_encoded['q3'] == 'Laurea Magistrale').astype(int)
    df_encoded['occup_studente'] = (df_encoded['q4'] == 'Studente/ Studentessa').astype(int)
    df_encoded['geo_centro'] = (df_encoded['q5'] == 'Centro').astype(int)
    df_encoded['reddito_alto'] = df_encoded['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
    
    # Likert chiave
    likert_vars = ['q8', 'q19', 'q21']
    for var in likert_vars:
        if var in df_encoded.columns:
            df_encoded[f'{var}_norm'] = (df_encoded[var] - 1) / 6

    som_vars = ['eta_norm', 'genere_donna', 'titolo_magistrale', 'occup_studente', 
                'geo_centro', 'reddito_alto'] + [f'{var}_norm' for var in likert_vars if var in df_original.columns]
    
    return df_encoded, som_vars

df_processed, som_vars = prepare_som_data()
X_som = df_processed[som_vars].fillna(df_processed[som_vars].mean()).values

print(f"Dataset SOM: {X_som.shape}")
print(f"Variabili: {som_vars}")

# Standardizzazione
scaler = StandardScaler()
X_som_scaled = scaler.fit_transform(X_som)

# Training SOM
som_weights = simple_som_implementation(X_som_scaled, som_width=6, som_height=6, 
                                       learning_rate=0.3, iterations=300)

print("Training SOM completato")

# Trova BMU per ogni osservazione
bmu_coordinates = find_bmu_for_samples(X_som_scaled, som_weights)

# Converti coordinate in cluster labels
som_labels = []
for coord in bmu_coordinates:
    cluster_id = coord[0] * 6 + coord[1]  # 6x6 grid
    som_labels.append(cluster_id)

som_labels = np.array(som_labels)
n_som_clusters = len(np.unique(som_labels))

print(f"SOM clusters naturali: {n_som_clusters}")
print(f"Distribuzione (primi 10): {np.bincount(som_labels)[:10]}")

# Crea meta-clusters raggruppando neuroni vicini
def create_som_metaclusters_simple(bmu_coords, target_k=3):
    """Crea meta-cluster usando K-means sulle coordinate BMU"""
    # Usa le coordinate dei BMU per clustering
    unique_coords = np.unique(bmu_coords, axis=0)
    
    if len(unique_coords) >= target_k:
        kmeans_meta = KMeans(n_clusters=target_k, random_state=42)
        meta_labels_unique = kmeans_meta.fit_predict(unique_coords)
        
        # Mappa ogni osservazione al meta-cluster
        coord_to_meta = {}
        for i, coord in enumerate(unique_coords):
            coord_to_meta[tuple(coord)] = meta_labels_unique[i]
        
        meta_labels = [coord_to_meta[tuple(coord)] for coord in bmu_coords]
        return np.array(meta_labels)
    else:
        # Fallback: usa le coordinate direttamente
        return bmu_coords[:, 0] % target_k

som_meta_labels = create_som_metaclusters_simple(bmu_coordinates, target_k=3)
print(f"Meta-cluster SOM: {len(np.unique(som_meta_labels))}")
print(f"Distribuzione meta-cluster: {np.bincount(som_meta_labels)}")

# Calcola metriche
if len(np.unique(som_meta_labels)) > 1:
    som_silhouette = silhouette_score(X_som_scaled, som_meta_labels)
    print(f"Silhouette SOM: {som_silhouette:.3f}")
else:
    som_silhouette = 0.0

# Confronto con K-means
kmeans_ref = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans_ref.fit_predict(X_som_scaled)

if len(np.unique(som_meta_labels)) > 1:
    ari_som_kmeans = adjusted_rand_score(som_meta_labels, kmeans_labels)
    print(f"ARI SOM vs K-means: {ari_som_kmeans:.3f}")
else:
    ari_som_kmeans = 0.0

# Visualizzazioni corrette
plt.figure(figsize=(15, 5))

# U-Matrix
plt.subplot(1, 3, 1)
u_matrix = create_distance_map(som_weights)
plt.imshow(u_matrix, cmap='bone_r', origin='lower')
plt.colorbar(label='Distanza media')
plt.title('SOM U-Matrix\n(Distanze tra neuroni)')
plt.xlabel('X')
plt.ylabel('Y')

# Density map
plt.subplot(1, 3, 2)
density_map = np.zeros((6, 6))
for coord in bmu_coordinates:
    density_map[coord[0], coord[1]] += 1

plt.imshow(density_map, cmap='viridis', origin='lower')
plt.colorbar(label='Frequenza')
plt.title('SOM Density Map\n(Attivazioni neuroni)')
plt.xlabel('X')
plt.ylabel('Y')

# Meta-cluster visualization
plt.subplot(1, 3, 3)
# Scatter plot delle coordinate BMU colorate per meta-cluster
colors = ['red', 'blue', 'green', 'orange', 'purple']
for meta_id in range(len(np.unique(som_meta_labels))):
    mask = som_meta_labels == meta_id
    coords_subset = bmu_coordinates[mask]
    plt.scatter(coords_subset[:, 1], coords_subset[:, 0], 
               c=colors[meta_id], label=f'Meta-cluster {meta_id}', 
               alpha=0.6, s=50)

plt.xlabel('X')
plt.ylabel('Y')
plt.title('SOM Meta-Clusters')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Profiling meta-cluster
print(f"\n=== PROFILING META-CLUSTER SOM ===")

for meta_id in range(len(np.unique(som_meta_labels))):
    mask = som_meta_labels == meta_id
    n_obs = np.sum(mask)
    
    if n_obs > 0:
        print(f"\nMETA-CLUSTER SOM {meta_id} ({n_obs} obs, {n_obs/len(som_meta_labels)*100:.1f}%):")
        
        cluster_profile = np.mean(X_som[mask], axis=0)
        
        for i, var_name in enumerate(som_vars):
            var_value = cluster_profile[i]
            if 'norm' in var_name or var_name == 'eta_norm':
                # Variabile normalizzata
                if var_name == 'eta_norm':
                    actual_value = var_value * (70-18) + 18
                    print(f"  {var_name}: {var_value:.3f} (≈{actual_value:.1f} anni)")
                else:
                    print(f"  {var_name}: {var_value:.3f} (scala 0-1)")
            else:
                # Variabile binaria
                pct = var_value * 100
                print(f"  {var_name}: {var_value:.3f} ({pct:.0f}%)")

# Valutazione finale
print(f"\n=== VALUTAZIONE SOM ===")

if som_silhouette > 0.3:
    som_quality = "Buona"
elif som_silhouette > 0.2:
    som_quality = "Accettabile"  
else:
    som_quality = "Limitata"

print(f"Qualità clustering: {som_quality} (Silhouette: {som_silhouette:.3f})")

if ari_som_kmeans > 0.6:
    novelty = "Simile a K-means"
elif ari_som_kmeans > 0.3:
    novelty = "Parzialmente diverso"
else:
    novelty = "Completamente diverso"

print(f"Novità vs K-means: {novelty} (ARI: {ari_som_kmeans:.3f})")

print(f"\n=== INSIGHTS SOM ===")
print("1. Topologia preservata: pattern spaziali visibili nella mappa")
print("2. U-matrix rivela regioni omogenee vs confini di transizione")  
print("3. Density map identifica profili più/meno comuni")
print("4. Approccio non-supervisionato con interpretazione geometrica")

print("\n=== MODULO SOM COMPLETATO ===")





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

print("=== MODULO FUZZY C-MEANS - MEMBERSHIP PROBABILISTICA ===")

def fuzzy_cmeans(data, n_clusters=3, m=2, max_iter=100, tol=1e-4, random_state=42):
    """
    Implementazione Fuzzy C-Means
    
    Parameters:
    - data: dataset (n_samples x n_features)
    - n_clusters: numero cluster
    - m: fuzziness parameter (m>1, tipicamente 2)
    - max_iter: iterazioni massime
    - tol: tolleranza convergenza
    """
    np.random.seed(random_state)
    n_samples, n_features = data.shape
    
    # Inizializza membership matrix U (n_samples x n_clusters)
    # Ogni riga somma a 1
    U = np.random.random((n_samples, n_clusters))
    U = U / U.sum(axis=1, keepdims=True)
    
    print(f"Inizializzando Fuzzy C-Means: {n_clusters} cluster, m={m}")
    
    for iteration in range(max_iter):
        U_old = U.copy()
        
        # Update cluster centers
        Um = U ** m  # Membership elevato a m
        centers = (Um.T @ data) / Um.sum(axis=0, keepdims=True).T
        
        # Update membership matrix
        for i in range(n_samples):
            for c in range(n_clusters):
                # Calcola distanze da tutti i centri
                distances = np.linalg.norm(data[i] - centers, axis=1)
                
                if distances[c] == 0:
                    # Punto coincide con centro
                    U[i] = 0
                    U[i, c] = 1
                else:
                    # Formula fuzzy c-means
                    denominator = 0
                    for j in range(n_clusters):
                        if distances[j] > 0:
                            denominator += (distances[c] / distances[j]) ** (2/(m-1))
                    
                    U[i, c] = 1 / denominator if denominator > 0 else 1
        
        # Normalizza membership (safety check)
        U = U / U.sum(axis=1, keepdims=True)
        
        # Check convergenza
        if np.max(np.abs(U - U_old)) < tol:
            print(f"Convergenza raggiunta all'iterazione {iteration + 1}")
            break
        
        if (iteration + 1) % 20 == 0:
            print(f"  Iterazione {iteration + 1}/{max_iter}")
    
    # Hard clustering: assegna ogni punto al cluster con membership massima
    hard_labels = np.argmax(U, axis=1)
    
    return centers, U, hard_labels

# Preparazione dataset (stesso dei moduli precedenti)
df_original = pd.read_excel('Questionario Sostenibilità 1 1.xlsx', 
                           sheet_name='Questionario Sostenibilità')

def prepare_fuzzy_data():
    df_encoded = df_original.copy()
    
    df_encoded['eta_norm'] = (df_encoded['q1'] - 18) / (70 - 18)
    df_encoded['genere_donna'] = (df_encoded['q2'] == 'Donna').astype(int)
    df_encoded['titolo_magistrale'] = (df_encoded['q3'] == 'Laurea Magistrale').astype(int)
    df_encoded['occup_studente'] = (df_encoded['q4'] == 'Studente/ Studentessa').astype(int)
    df_encoded['geo_centro'] = (df_encoded['q5'] == 'Centro').astype(int)
    df_encoded['reddito_alto'] = df_encoded['q6'].isin(['30001 - 50000', 'Più di 50000']).astype(int)
    
    # Likert attitudes
    likert_vars = ['q8', 'q19', 'q20', 'q21']
    for var in likert_vars:
        if var in df_encoded.columns:
            df_encoded[f'{var}_norm'] = (df_encoded[var] - 1) / 6

    fuzzy_vars = ['eta_norm', 'genere_donna', 'titolo_magistrale', 'occup_studente', 
                  'geo_centro', 'reddito_alto'] + [f'{var}_norm' for var in likert_vars if var in df_original.columns]
    
    return df_encoded, fuzzy_vars

df_processed, fuzzy_vars = prepare_fuzzy_data()
X_fuzzy = df_processed[fuzzy_vars].fillna(df_processed[fuzzy_vars].mean()).values

print(f"Dataset Fuzzy C-Means: {X_fuzzy.shape}")
print(f"Variabili: {fuzzy_vars}")

# Standardizzazione
scaler = StandardScaler()
X_fuzzy_scaled = scaler.fit_transform(X_fuzzy)

# Esegui Fuzzy C-Means
centers, membership_matrix, hard_labels = fuzzy_cmeans(
    X_fuzzy_scaled, n_clusters=3, m=2.0, max_iter=100
)

print(f"Fuzzy C-Means completato")
print(f"Distribuzione hard clustering: {np.bincount(hard_labels)}")

# Analisi membership
print(f"\n=== ANALISI MEMBERSHIP FUZZY ===")

# Statistiche membership
membership_max = np.max(membership_matrix, axis=1)
membership_entropy = -np.sum(membership_matrix * np.log(membership_matrix + 1e-10), axis=1)

print(f"Membership massima: {np.mean(membership_max):.3f} ± {np.std(membership_max):.3f}")
print(f"Entropy media: {np.mean(membership_entropy):.3f} ± {np.std(membership_entropy):.3f}")

# Identifica osservazioni "fuzzy" (bassa membership massima)
fuzzy_threshold = 0.5
fuzzy_cases = membership_max < fuzzy_threshold
n_fuzzy = np.sum(fuzzy_cases)

print(f"Casi 'fuzzy' (max membership < {fuzzy_threshold}): {n_fuzzy} ({n_fuzzy/len(hard_labels)*100:.1f}%)")

# Identifica casi con membership alta (>0.8)
crisp_threshold = 0.8
crisp_cases = membership_max > crisp_threshold
n_crisp = np.sum(crisp_cases)

print(f"Casi 'crisp' (max membership > {crisp_threshold}): {n_crisp} ({n_crisp/len(hard_labels)*100:.1f}%)")

# Qualità clustering
if len(np.unique(hard_labels)) > 1:
    fuzzy_silhouette = silhouette_score(X_fuzzy_scaled, hard_labels)
    print(f"Silhouette Fuzzy C-Means: {fuzzy_silhouette:.3f}")
else:
    fuzzy_silhouette = 0.0

# Confronto con K-means
kmeans_ref = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans_ref.fit_predict(X_fuzzy_scaled)

ari_fuzzy_kmeans = adjusted_rand_score(hard_labels, kmeans_labels)
print(f"ARI Fuzzy vs K-means: {ari_fuzzy_kmeans:.3f}")

# Visualizzazioni
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

# 1. Membership heatmap
im1 = ax1.imshow(membership_matrix.T, aspect='auto', cmap='viridis')
ax1.set_title('Membership Matrix Fuzzy C-Means')
ax1.set_xlabel('Osservazioni')
ax1.set_ylabel('Cluster')
plt.colorbar(im1, ax=ax1, label='Membership')

# 2. Distribuzione membership massima
ax2.hist(membership_max, bins=20, alpha=0.7, edgecolor='black')
ax2.axvline(fuzzy_threshold, color='red', linestyle='--', label=f'Soglia fuzzy ({fuzzy_threshold})')
ax2.axvline(crisp_threshold, color='green', linestyle='--', label=f'Soglia crisp ({crisp_threshold})')
ax2.set_xlabel('Membership Massima')
ax2.set_ylabel('Frequenza')
ax2.set_title('Distribuzione Membership Massima')
ax2.legend()

# 3. Entropy vs Membership massima
scatter = ax3.scatter(membership_max, membership_entropy, c=hard_labels, alpha=0.7, cmap='Set1')
ax3.set_xlabel('Membership Massima')
ax3.set_ylabel('Entropy')
ax3.set_title('Membership vs Uncertainty')
plt.colorbar(scatter, ax=ax3, label='Cluster')

# 4. Confronto clustering methods (PCA projection)
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_fuzzy_scaled)

colors_fuzzy = ['red', 'blue', 'green']
colors_kmeans = ['pink', 'lightblue', 'lightgreen']

for i in range(3):
    # Fuzzy clustering
    mask_f = hard_labels == i
    ax4.scatter(X_pca[mask_f, 0], X_pca[mask_f, 1], c=colors_fuzzy[i], 
               alpha=0.7, s=50, label=f'Fuzzy {i}', marker='o')
    
    # K-means clustering
    mask_k = kmeans_labels == i
    ax4.scatter(X_pca[mask_k, 0], X_pca[mask_k, 1], c=colors_kmeans[i], 
               alpha=0.3, s=20, label=f'K-means {i}', marker='x')

ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
ax4.set_title('Fuzzy vs K-means (PCA space)')
ax4.legend()

plt.tight_layout()
plt.show()

# Profiling cluster con membership
print(f"\n=== PROFILING CLUSTER FUZZY ===")

for cluster_id in range(3):
    mask = hard_labels == cluster_id
    n_obs = np.sum(mask)
    
    print(f"\nCLUSTER FUZZY {cluster_id} ({n_obs} obs, {n_obs/len(hard_labels)*100:.1f}%):")
    
    # Membership media per questo cluster
    cluster_memberships = membership_matrix[mask, cluster_id]
    avg_membership = np.mean(cluster_memberships)
    membership_certainty = np.sum(cluster_memberships > 0.8) / len(cluster_memberships) * 100
    
    print(f"  Membership media: {avg_membership:.3f}")
    print(f"  Casi certi (>0.8): {membership_certainty:.0f}%")
    
    # Profilo variabili
    cluster_profile = np.mean(X_fuzzy[mask], axis=0)
    
    for i, var_name in enumerate(fuzzy_vars):
        var_value = cluster_profile[i]
        if 'norm' in var_name:
            if var_name == 'eta_norm':
                actual_age = var_value * (70-18) + 18
                print(f"  {var_name}: {var_value:.3f} (≈{actual_age:.1f} anni)")
            else:
                likert_actual = var_value * 6 + 1
                print(f"  {var_name}: {var_value:.3f} (≈{likert_actual:.1f}/7)")
        else:
            pct = var_value * 100
            print(f"  {var_name}: {var_value:.3f} ({pct:.0f}%)")

# Analisi transizioni (casi fuzzy)
print(f"\n=== ANALISI CASI FUZZY (membership < {fuzzy_threshold}) ===")

if n_fuzzy > 0:
    fuzzy_indices = np.where(fuzzy_cases)[0]
    
    print(f"Esempi di casi 'in transizione':")
    for i in range(min(5, len(fuzzy_indices))):  # Max 5 esempi
        idx = fuzzy_indices[i]
        memberships = membership_matrix[idx]
        primary_cluster = np.argmax(memberships)
        secondary_cluster = np.argsort(memberships)[-2]
        
        print(f"  Caso {idx}: Cluster {primary_cluster} ({memberships[primary_cluster]:.3f}) "
              f"vs Cluster {secondary_cluster} ({memberships[secondary_cluster]:.3f})")
        
        # Profilo demografico del caso fuzzy
        case_profile = X_fuzzy[idx]
        age = case_profile[0] * (70-18) + 18
        gender = "Donna" if case_profile[1] > 0.5 else "Uomo"
        student = "Studente" if case_profile[3] > 0.5 else "Non-studente"
        
        print(f"    Profilo: {gender}, {age:.0f} anni, {student}")

# Valutazione finale
print(f"\n=== VALUTAZIONE FUZZY C-MEANS ===")

if fuzzy_silhouette > 0.3:
    fuzzy_quality = "Buona"
elif fuzzy_silhouette > 0.2:
    fuzzy_quality = "Accettabile"
else:
    fuzzy_quality = "Limitata"

print(f"Qualità clustering: {fuzzy_quality} (Silhouette: {fuzzy_silhouette:.3f})")
print(f"Coerenza con K-means: {ari_fuzzy_kmeans:.3f}")
print(f"Fuzziness: {n_fuzzy} casi ambigui, {n_crisp} casi certi")

print(f"\n=== VANTAGGI FUZZY C-MEANS ===")
print("1. Membership probabilistica: cattura incertezza classificazione")
print("2. Identifica individui 'in transizione' tra comportamenti")
print("3. Quantifica confidence nelle assegnazioni cluster") 
print("4. Utile per targeting graduato (non solo binario)")

print(f"\n=== INSIGHTS SOSTENIBILITÀ ===")
print(f"- {n_fuzzy} persone ({n_fuzzy/len(hard_labels)*100:.1f}%) mostrano pattern misti")
print(f"- {n_crisp} persone ({n_crisp/len(hard_labels)*100:.1f}%) hanno profili chiari")
print("- I casi fuzzy sono candidati per interventi personalizzati")
print("- Membership graduale permette segmentazione più sofisticata")

print("\n=== MODULO FUZZY C-MEANS COMPLETATO ===")
